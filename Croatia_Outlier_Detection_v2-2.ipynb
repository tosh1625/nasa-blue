{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "991b0852a5dc4625aa278d021e6839c1": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_1bdfd8c4812d4106b044b48788f299fa",
            "msg_id": "",
            "outputs": [
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "Fitting: \u001b[38;2;23;100;244m━━━━\u001b[0m\u001b[38;5;237m╺\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m 11%\u001b[0m 0:00:34 Average Loss = 7,971.8\n",
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Fitting: <span style=\"color: #1764f4; text-decoration-color: #1764f4\">━━━━</span><span style=\"color: #3a3a3a; text-decoration-color: #3a3a3a\">╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span> <span style=\"color: #800080; text-decoration-color: #800080\"> 11%</span> 0:00:34 Average Loss = 7,971.8\n</pre>\n"
                },
                "metadata": {}
              }
            ]
          }
        },
        "1bdfd8c4812d4106b044b48788f299fa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c3eb31b413b54ba886a157b0c6b67a7f": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_f07fade6f7734bd6841c1de1255177ef",
            "msg_id": "",
            "outputs": [
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "                                                                                                                   \n \u001b[1m \u001b[0m\u001b[1mProgress                 \u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1mDraws\u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1mDivergences\u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1mStep size\u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1mGrad evals\u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1mSampling Speed\u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1mElapsed\u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1mRemaining\u001b[0m\u001b[1m \u001b[0m \n ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n  \u001b[38;2;214;39;40m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m   4500    29            0.05        63           69.87 draws/s    0:01:04   0:00:00    \n  \u001b[38;2;214;39;40m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m   4500    21            0.05        127          32.94 draws/s    0:02:16   0:00:00    \n  \u001b[38;2;214;39;40m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m   4500    17            0.04        127          21.55 draws/s    0:03:28   0:00:00    \n  \u001b[38;2;214;39;40m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m   4500    22            0.05        127          16.54 draws/s    0:04:32   0:00:00    \n                                                                                                                   \n",
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">                                                                                                                   \n <span style=\"font-weight: bold\"> Progress                  </span> <span style=\"font-weight: bold\"> Draws </span> <span style=\"font-weight: bold\"> Divergences </span> <span style=\"font-weight: bold\"> Step size </span> <span style=\"font-weight: bold\"> Grad evals </span> <span style=\"font-weight: bold\"> Sampling Speed </span> <span style=\"font-weight: bold\"> Elapsed </span> <span style=\"font-weight: bold\"> Remaining </span> \n ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n  <span style=\"color: #d62728; text-decoration-color: #d62728\">━━━━━━━━━━━━━━━━━━━━━━━━━</span>   4500    29            0.05        63           69.87 draws/s    0:01:04   0:00:00    \n  <span style=\"color: #d62728; text-decoration-color: #d62728\">━━━━━━━━━━━━━━━━━━━━━━━━━</span>   4500    21            0.05        127          32.94 draws/s    0:02:16   0:00:00    \n  <span style=\"color: #d62728; text-decoration-color: #d62728\">━━━━━━━━━━━━━━━━━━━━━━━━━</span>   4500    17            0.04        127          21.55 draws/s    0:03:28   0:00:00    \n  <span style=\"color: #d62728; text-decoration-color: #d62728\">━━━━━━━━━━━━━━━━━━━━━━━━━</span>   4500    22            0.05        127          16.54 draws/s    0:04:32   0:00:00    \n                                                                                                                   \n</pre>\n"
                },
                "metadata": {}
              }
            ]
          }
        },
        "f07fade6f7734bd6841c1de1255177ef": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1a577c0e27fd43ef9cbd2eb36e19b5d6": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_59c9df66fdda49ca8ee682805e7d8679",
            "msg_id": "",
            "outputs": [
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "Sampling ... \u001b[32m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m100%\u001b[0m 0:00:00 / 0:00:01\n",
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Sampling ... <span style=\"color: #008000; text-decoration-color: #008000\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span> <span style=\"color: #800080; text-decoration-color: #800080\">100%</span> 0:00:00 / 0:00:01\n</pre>\n"
                },
                "metadata": {}
              }
            ]
          }
        },
        "59c9df66fdda49ca8ee682805e7d8679": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "883e02307938440cbac30ebc71585823": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_79f88480febc49ed91c38e2d1fc45a47",
            "msg_id": "",
            "outputs": [
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "Fitting: \u001b[38;2;23;100;244m━\u001b[0m\u001b[38;5;237m╺\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m  3%\u001b[0m 0:00:23 Average Loss = 112.56\n",
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Fitting: <span style=\"color: #1764f4; text-decoration-color: #1764f4\">━</span><span style=\"color: #3a3a3a; text-decoration-color: #3a3a3a\">╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span> <span style=\"color: #800080; text-decoration-color: #800080\">  3%</span> 0:00:23 Average Loss = 112.56\n</pre>\n"
                },
                "metadata": {}
              }
            ]
          }
        },
        "79f88480febc49ed91c38e2d1fc45a47": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b736b84e51a84a3ead09e6e2f37b5bcd": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_89d3aa776d144421a030a05f22e2193b",
            "msg_id": "",
            "outputs": [
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "                                                                                                                   \n \u001b[1m \u001b[0m\u001b[1mProgress                 \u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1mDraws\u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1mDivergences\u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1mStep size\u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1mGrad evals\u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1mSampling Speed\u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1mElapsed\u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1mRemaining\u001b[0m\u001b[1m \u001b[0m \n ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n  \u001b[38;2;31;119;180m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m   4500    0             0.25        15           634.08 draws/s   0:00:07   0:00:00    \n  \u001b[38;2;31;119;180m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m   4500    0             0.29        15           315.39 draws/s   0:00:14   0:00:00    \n  \u001b[38;2;31;119;180m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m   4500    0             0.26        15           206.24 draws/s   0:00:21   0:00:00    \n  \u001b[38;2;31;119;180m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m   4500    0             0.25        15           156.01 draws/s   0:00:28   0:00:00    \n                                                                                                                   \n",
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">                                                                                                                   \n <span style=\"font-weight: bold\"> Progress                  </span> <span style=\"font-weight: bold\"> Draws </span> <span style=\"font-weight: bold\"> Divergences </span> <span style=\"font-weight: bold\"> Step size </span> <span style=\"font-weight: bold\"> Grad evals </span> <span style=\"font-weight: bold\"> Sampling Speed </span> <span style=\"font-weight: bold\"> Elapsed </span> <span style=\"font-weight: bold\"> Remaining </span> \n ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n  <span style=\"color: #1f77b4; text-decoration-color: #1f77b4\">━━━━━━━━━━━━━━━━━━━━━━━━━</span>   4500    0             0.25        15           634.08 draws/s   0:00:07   0:00:00    \n  <span style=\"color: #1f77b4; text-decoration-color: #1f77b4\">━━━━━━━━━━━━━━━━━━━━━━━━━</span>   4500    0             0.29        15           315.39 draws/s   0:00:14   0:00:00    \n  <span style=\"color: #1f77b4; text-decoration-color: #1f77b4\">━━━━━━━━━━━━━━━━━━━━━━━━━</span>   4500    0             0.26        15           206.24 draws/s   0:00:21   0:00:00    \n  <span style=\"color: #1f77b4; text-decoration-color: #1f77b4\">━━━━━━━━━━━━━━━━━━━━━━━━━</span>   4500    0             0.25        15           156.01 draws/s   0:00:28   0:00:00    \n                                                                                                                   \n</pre>\n"
                },
                "metadata": {}
              }
            ]
          }
        },
        "89d3aa776d144421a030a05f22e2193b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4a0d6c10fec4490ab5d90c6eddb83a52": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_3611b2bd94f34dc7990d53c8f07c924c",
            "msg_id": "",
            "outputs": [
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "Sampling ... \u001b[32m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m100%\u001b[0m 0:00:00 / 0:00:00\n",
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Sampling ... <span style=\"color: #008000; text-decoration-color: #008000\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span> <span style=\"color: #800080; text-decoration-color: #800080\">100%</span> 0:00:00 / 0:00:00\n</pre>\n"
                },
                "metadata": {}
              }
            ]
          }
        },
        "3611b2bd94f34dc7990d53c8f07c924c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install haversine cartopy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jriroy4c04SD",
        "outputId": "3adaafb2-697d-4bc5-9493-19265985ad0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting haversine\n",
            "  Downloading haversine-2.9.0-py2.py3-none-any.whl.metadata (5.8 kB)\n",
            "Collecting cartopy\n",
            "  Downloading Cartopy-0.24.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.9 kB)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.11/dist-packages (from cartopy) (2.0.2)\n",
            "Requirement already satisfied: matplotlib>=3.6 in /usr/local/lib/python3.11/dist-packages (from cartopy) (3.10.0)\n",
            "Requirement already satisfied: shapely>=1.8 in /usr/local/lib/python3.11/dist-packages (from cartopy) (2.1.0)\n",
            "Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.11/dist-packages (from cartopy) (24.2)\n",
            "Requirement already satisfied: pyshp>=2.3 in /usr/local/lib/python3.11/dist-packages (from cartopy) (2.3.1)\n",
            "Requirement already satisfied: pyproj>=3.3.1 in /usr/local/lib/python3.11/dist-packages (from cartopy) (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->cartopy) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->cartopy) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->cartopy) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->cartopy) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->cartopy) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->cartopy) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->cartopy) (2.8.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from pyproj>=3.3.1->cartopy) (2025.1.31)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.6->cartopy) (1.17.0)\n",
            "Downloading haversine-2.9.0-py2.py3-none-any.whl (7.7 kB)\n",
            "Downloading Cartopy-0.24.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.7/11.7 MB\u001b[0m \u001b[31m110.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: haversine, cartopy\n",
            "Successfully installed cartopy-0.24.1 haversine-2.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OLqe-D1M1Ue7",
        "outputId": "54ee4f16-a7cb-4f8e-8ee3-2ed78be5c83d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries (ensure all are present)\n",
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import re\n",
        "from sklearn.cluster import DBSCAN\n",
        "from haversine import haversine, Unit\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "from typing import Dict, List, Tuple, Optional, Any, Literal\n",
        "\n",
        "# Suppress SettingWithCopyWarning, use .loc for assignments to avoid it where possible\n",
        "pd.options.mode.chained_assignment = None  # default='warn'\n",
        "\n",
        "# Bayesian analysis libraries\n",
        "try:\n",
        "    import pymc as pm\n",
        "    import arviz as az\n",
        "    print(f\"PyMC version: {pm.__version__}\")\n",
        "    print(f\"ArviZ version: {az.__version__}\")\n",
        "    BAYESIAN_ENABLED = True\n",
        "except ImportError:\n",
        "    print(\"PyMC or ArviZ not found. Bayesian analysis section will be skipped.\")\n",
        "    print(\"Install with: pip install pymc arviz\")\n",
        "    BAYESIAN_ENABLED = False\n",
        "\n",
        "# Mapping libraries\n",
        "try:\n",
        "    import cartopy.crs as ccrs\n",
        "    import cartopy.feature as cfeature\n",
        "    CARTOPY_AVAILABLE = True\n",
        "    print(\"Cartopy found, map plotting enabled.\")\n",
        "except ImportError:\n",
        "    print(\"Cartopy not found. Map visualization of outliers will be skipped.\")\n",
        "    print(\"Install with: conda install cartopy or pip install cartopy\")\n",
        "    CARTOPY_AVAILABLE = False\n",
        "\n",
        "\n",
        "# --- Configuration ---\n",
        "FOLDER_PATH = '/content/drive/MyDrive/Datasheets'\n",
        "OUTPUT_DIR = \"/content/analysis_results_split_source_bayes_map\" # Updated output dir\n",
        "CLUSTER_RADIUS_KM = 10\n",
        "ROLLING_WINDOW_DAYS = 5\n",
        "DATASET_PREFIX = 'globe'.lower() # Or 'seabass'\n",
        "\n",
        "# Define the column name for water body source\n",
        "WATER_BODY_SOURCE_COL = 'water_body_source(hes_yellow)'\n",
        "TARGET_WATER_SOURCES = ['River_Stream', 'Pond_Lake'] # Case-sensitive match to data\n",
        "\n",
        "CROATIA_BOUNDING_BOX = {\n",
        "    'min_lat': 42.4,\n",
        "    'max_lat': 46.5,\n",
        "    'min_lon': 13.5,\n",
        "    'max_lon': 19.4\n",
        "}\n",
        "\n",
        "# Bayesian Analysis Configuration\n",
        "RUN_BAYESIAN_ANALYSIS = BAYESIAN_ENABLED # Only run if libraries are loaded\n",
        "BAYESIAN_TARGET_KEYS = ['tube_river_stream', 'disk_pond_lake'] # Specify which results to analyze\n",
        "                                            # Use ['all'] to attempt all generated keys, or specific list\n",
        "BAYESIAN_N_DRAWS = 2000\n",
        "BAYESIAN_N_TUNE = 2500\n",
        "BAYESIAN_N_CHAINS = 4\n",
        "BAYESIAN_TARGET_ACCEPT = 0.90\n",
        "BAYESIAN_PPC_HDI_PROB = 0.99 # HDI probability for identifying within-cluster outliers\n",
        "\n",
        "# --- End Configuration ---\n",
        "\n",
        "# Mount Google Drive\n",
        "# try:\n",
        "#     drive.mount('/content/drive')\n",
        "# except Exception as e:\n",
        "#     print(f\"Error mounting Google Drive: {e}\")\n",
        "#     pass\n",
        "\n",
        "# Create output directory if it doesn't exist\n",
        "if not os.path.exists(OUTPUT_DIR):\n",
        "    os.makedirs(OUTPUT_DIR)\n",
        "    print(f\"Created output directory: {OUTPUT_DIR}\")\n",
        "\n",
        "# --- Helper Functions ---\n",
        "def find_newest_dataset(folder_path: str, dataset_prefix: str) -> Optional[str]:\n",
        "    \"\"\"\n",
        "    Finds the newest dataset file in the specified folder matching the prefix\n",
        "    and a YYYYMMDD date pattern.\n",
        "    \"\"\"\n",
        "    csv_files = []\n",
        "    pattern = re.compile(rf'^{re.escape(dataset_prefix)}_(\\d{{8}})\\.csv$')\n",
        "    try:\n",
        "        if not os.path.isdir(folder_path):\n",
        "            print(f\"Error: Folder not found at {folder_path}\")\n",
        "            return None\n",
        "        for filename in os.listdir(folder_path):\n",
        "            match = pattern.match(filename)\n",
        "            if match:\n",
        "                csv_files.append((filename, int(match.group(1))))\n",
        "        if not csv_files:\n",
        "            print(f\"No CSV files starting with '{dataset_prefix}_YYYYMMDD' found in {folder_path}\")\n",
        "            return None\n",
        "        csv_files.sort(key=lambda item: item[1], reverse=True)\n",
        "        newest_file_name = csv_files[0][0]\n",
        "        print(f\"Found {len(csv_files)} file(s). Using the newest: '{newest_file_name}'.\")\n",
        "        return os.path.join(folder_path, newest_file_name)\n",
        "    except Exception as e:\n",
        "        print(f\"Error finding dataset files in {folder_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "# --- Helper Functions ---\n",
        "def load_data(file_path: str) -> Optional[pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Loads data from a CSV file into a pandas DataFrame.\n",
        "    Checks for essential columns including the new water body source column.\n",
        "    \"\"\"\n",
        "    required_columns = [\n",
        "        'latitude(sample)', 'longitude(sample)', 'measured_on',\n",
        "        WATER_BODY_SOURCE_COL, # Check for the specific water source column\n",
        "        # Transparency columns (at least one needed)\n",
        "        'transparencies:transparencydiskimagedisappearance(m)',\n",
        "        'transparencies:tubeimagedisappearance(cm)',\n",
        "        'transparencies:sensorturbidityntu',\n",
        "        # Other (not used in core logic yet)\n",
        "        # 'depth(hes_yellow)',\n",
        "        # 'source_of_distance_from_water(hes_yellow)',\n",
        "        # 'estimated_tube_length(hes_yellow)',\n",
        "        # 'water_body_type'\n",
        "    ]\n",
        "    essential_cols = ['latitude(sample)', 'longitude(sample)', 'measured_on', WATER_BODY_SOURCE_COL]\n",
        "    transparency_cols = [\n",
        "        'transparencies:transparency disk image disappearance (m)',\n",
        "        'transparencies:tube image disappearance (cm)',\n",
        "        'transparencies:sensor turbidity ntu'\n",
        "    ]\n",
        "\n",
        "    try:\n",
        "        df = pd.read_csv(file_path, low_memory=False)\n",
        "        print(f\"Successfully imported '{os.path.basename(file_path)}'. Shape: {df.shape}\")\n",
        "\n",
        "        # Check essential columns needed for filtering/grouping/splitting\n",
        "        missing_essential_cols = [col for col in essential_cols if col not in df.columns]\n",
        "        if missing_essential_cols:\n",
        "            print(f\"Error: Missing essential columns for analysis: {missing_essential_cols}\")\n",
        "            return None\n",
        "\n",
        "        # Check if *at least one* transparency column exists.\n",
        "        transparency_cols_present = [col for col in transparency_cols if col in df.columns]\n",
        "        if not transparency_cols_present:\n",
        "             print(f\"Warning: No recognized transparency columns found (disk, tube, sensor). Analysis may yield limited results.\")\n",
        "\n",
        "        # Check for other required/new columns (optional)\n",
        "        missing_other_cols = [col for col in required_columns if col not in df.columns and col not in essential_cols and col not in transparency_cols]\n",
        "        if missing_other_cols:\n",
        "            print(f\"Warning: Some expected columns are missing but not essential for core logic: {missing_other_cols}\")\n",
        "\n",
        "        return df\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: File not found at {file_path}\")\n",
        "        return None\n",
        "    except pd.errors.EmptyDataError:\n",
        "        print(f\"Error: The file '{os.path.basename(file_path)}' is empty.\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"Error importing '{os.path.basename(file_path)}': {e}\")\n",
        "        return None\n",
        "\n",
        "# --- Step 1 ---\n",
        "def filter_croatia_data(df: pd.DataFrame, bbox: Dict[str, float]) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Filters the DataFrame to include only data points within the specified bounding box.\n",
        "    Also ensures coordinate columns are numeric.\n",
        "    \"\"\"\n",
        "    print(\"Step 1: Filtering data for Croatia...\")\n",
        "    df['latitude(sample)'] = pd.to_numeric(df['latitude(sample)'], errors='coerce')\n",
        "    df['longitude(sample)'] = pd.to_numeric(df['longitude(sample)'], errors='coerce')\n",
        "\n",
        "    original_count = len(df)\n",
        "    df.dropna(subset=['latitude(sample)', 'longitude(sample)'], inplace=True)\n",
        "    if original_count > len(df):\n",
        "        print(f\"  - Dropped {original_count - len(df)} rows with missing/invalid coordinates.\")\n",
        "\n",
        "    croatia_df = df[\n",
        "        (df['latitude(sample)'] >= bbox['min_lat']) & (df['latitude(sample)'] <= bbox['max_lat']) &\n",
        "        (df['longitude(sample)'] >= bbox['min_lon']) & (df['longitude(sample)'] <= bbox['max_lon'])\n",
        "    ]\n",
        "    print(f\"  - Found {len(croatia_df)} data points within Croatia's bounding box.\")\n",
        "    return croatia_df.copy()\n",
        "\n",
        "\n",
        "# --- Step 2: Determine Primary Measurement ---\n",
        "def determine_primary_measurement(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Adds a 'primary_measurement_type' column based on available non-null transparency data.\n",
        "    Priority: disk > tube > sensor.\n",
        "    \"\"\"\n",
        "    print(\"Step 2: Determining primary measurement type for each row...\")\n",
        "    conditions = []\n",
        "    choices = []\n",
        "    disk_col = 'transparencies:transparency disk image disappearance (m)'\n",
        "    tube_col = 'transparencies:tube image disappearance (cm)'\n",
        "    sensor_col = 'transparencies:sensor turbidity ntu'\n",
        "\n",
        "    # Check if columns exist before attempting to use them\n",
        "    if disk_col in df.columns:\n",
        "        df[disk_col] = pd.to_numeric(df[disk_col], errors='coerce')\n",
        "        conditions.append(df[disk_col].notna())\n",
        "        choices.append('disk')\n",
        "    else: # Print Statements for Debugging\n",
        "        print(f\"  - Debug: Disk column '{disk_col}' not found.\")\n",
        "\n",
        "    if tube_col in df.columns:\n",
        "        df[tube_col] = pd.to_numeric(df[tube_col], errors='coerce')\n",
        "        conditions.append(df[tube_col].notna())\n",
        "        choices.append('tube')\n",
        "    else:\n",
        "        print(f\"  - Debug: Tube column '{tube_col}' not found.\")\n",
        "\n",
        "    if sensor_col in df.columns:\n",
        "        df[sensor_col] = pd.to_numeric(df[sensor_col], errors='coerce')\n",
        "        conditions.append(df[sensor_col].notna())\n",
        "        choices.append('sensor')\n",
        "    else:\n",
        "        print(f\"  - Debug: Sensor column '{sensor_col}' not found.\")\n",
        "\n",
        "    # Check if conditions list is empty BEFORE calling np.select\n",
        "    if not conditions:\n",
        "        print(\"  - ERROR: No valid transparency columns found or processed. Cannot determine primary measurement.\")\n",
        "        # Assign 'none' to all rows if no columns were found/valid\n",
        "        df['primary_measurement_type'] = 'none'\n",
        "    else:\n",
        "        df['primary_measurement_type'] = np.select(conditions, choices, default='none')\n",
        "\n",
        "    counts = df['primary_measurement_type'].value_counts()\n",
        "    print(\"  - Primary measurement type counts:\")\n",
        "    for Mtype, count in counts.items():\n",
        "        print(f\"    - {Mtype}: {count}\")\n",
        "    return df\n",
        "\n",
        "# --- Step 3: Separate by Measurement Type ---\n",
        "def separate_by_measurement_type(df: pd.DataFrame) -> Dict[str, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Separates the DataFrame into a dictionary based on 'primary_measurement_type'.\n",
        "    Focuses on 'disk' and 'tube'.\n",
        "    \"\"\"\n",
        "    print(\"Step 3: Separating data by primary measurement type ('disk', 'tube')...\")\n",
        "    measurement_dfs = {}\n",
        "    target_types = ['disk', 'tube']\n",
        "    for m_type in target_types:\n",
        "        subset_df = df[df['primary_measurement_type'] == m_type].copy()\n",
        "        if not subset_df.empty:\n",
        "             measurement_dfs[m_type] = subset_df\n",
        "             print(f\"  - Measurement type '{m_type}': {len(subset_df)} data points\")\n",
        "        else:\n",
        "             print(f\"  - Measurement type '{m_type}': 0 data points\")\n",
        "    return measurement_dfs\n",
        "\n",
        "\n",
        "# --- Step 4: Separate by Water Body Source ---\n",
        "def separate_by_water_source(measurement_df: pd.DataFrame, measurement_label: str) -> Dict[str, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Separates a measurement-specific DataFrame by the WATER_BODY_SOURCE_COL.\n",
        "    Uses safe keys (e.g., 'River_Stream' -> 'river_stream').\n",
        "    \"\"\"\n",
        "    print(f\"Step 4 [{measurement_label}]: Separating by water body source ('{WATER_BODY_SOURCE_COL}')...\")\n",
        "    # Ensure the source column exists and handle NaNs\n",
        "    if WATER_BODY_SOURCE_COL not in measurement_df.columns:\n",
        "        print(f\"  - ERROR: Water body source column '{WATER_BODY_SOURCE_COL}' not found in data for {measurement_label}. Skipping separation.\")\n",
        "        return {}\n",
        "    measurement_df[WATER_BODY_SOURCE_COL] = measurement_df[WATER_BODY_SOURCE_COL].astype(str).fillna('unknown')\n",
        "\n",
        "    water_sources = measurement_df[WATER_BODY_SOURCE_COL].unique()\n",
        "    print(f\"  - Water body sources found for '{measurement_label}': {', '.join(map(str, water_sources))}\")\n",
        "    water_source_dfs = {}\n",
        "    for water_source in water_sources:\n",
        "        # Create a safe key (lowercase, replace non-alphanumeric with underscore)\n",
        "        safe_water_source_label = re.sub(r'\\W+', '_', str(water_source).lower())\n",
        "        if not safe_water_source_label: safe_water_source_label = 'unknown'\n",
        "\n",
        "        df_subset = measurement_df[measurement_df[WATER_BODY_SOURCE_COL] == water_source].copy()\n",
        "        if not df_subset.empty:\n",
        "             water_source_dfs[safe_water_source_label] = df_subset\n",
        "             print(f\"    - '{water_source}' (key: '{safe_water_source_label}'): {len(df_subset)} data points\")\n",
        "        else:\n",
        "             # This case might not happen if unique() only returns present values\n",
        "             pass\n",
        "\n",
        "    return water_source_dfs\n",
        "\n",
        "\n",
        "# --- Step 5: Cluster by Location ---\n",
        "def cluster_by_location(\n",
        "    water_source_dfs: Dict[str, pd.DataFrame],\n",
        "    eps_km: float,\n",
        "    measurement_label: str,\n",
        "    water_source_label: str,\n",
        "    output_dir: str\n",
        ") -> Dict[str, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Clusters data points by spatial proximity. Expects single item in dict matching water_source_label.\n",
        "    Logs updated labels.\n",
        "    *** ADDED: Saves a CSV mapping site_id to cluster label. ***\n",
        "    \"\"\"\n",
        "    print(f\"Step 5 [{measurement_label} / {water_source_label}]: Clustering by location (eps={eps_km} km)...\")\n",
        "    clustered_dfs = {}\n",
        "    site_id_col = 'site_id'\n",
        "\n",
        "    def haversine_distance_sklearn(coord1, coord2):\n",
        "        return haversine((coord1[0], coord1[1]), (coord2[0], coord2[1]), unit=Unit.KILOMETERS)\n",
        "\n",
        "    if len(water_source_dfs) != 1 or list(water_source_dfs.keys())[0] != water_source_label:\n",
        "         print(f\"  - ERROR: cluster_by_location expected dict with single key '{water_source_label}', but got keys {list(water_source_dfs.keys())}. Aborting clustering.\")\n",
        "         return water_source_dfs # Return original dict to avoid breaking flow\n",
        "\n",
        "    df = water_source_dfs[water_source_label]\n",
        "\n",
        "    # --- Check if site_id column exists before proceeding ---\n",
        "    if site_id_col not in df.columns:\n",
        "        print(f\"  - Warning: Column '{site_id_col}' not found. Cannot save site-to-cluster mapping CSV.\")\n",
        "        save_mapping_possible = False\n",
        "    else:\n",
        "        save_mapping_possible = True\n",
        "        original_len_sid = len(df)\n",
        "        df.dropna(subset=[site_id_col], inplace=True)\n",
        "        if len(df) < original_len_sid:\n",
        "             print(f\"  - Dropped {original_len_sid - len(df)} rows with missing '{site_id_col}'.\")\n",
        "\n",
        "    if len(df) < 2:\n",
        "        print(f\"  - Not enough data points (< 2) for clustering. Assigning all to cluster -1 (noise).\")\n",
        "        df['cluster'] = -1\n",
        "        clustered_dfs[water_source_label] = df\n",
        "        # Attempt to save mapping even if clustering isn't meaningful (all -1)\n",
        "        if save_mapping_possible and not df.empty:\n",
        "            try:\n",
        "                map_df = df[[site_id_col, 'cluster']]\n",
        "                safe_key = f\"{measurement_label}_{water_source_label}\"\n",
        "                map_filename = os.path.join(output_dir, f\"site_cluster_map_{safe_key}.csv\")\n",
        "                map_df.to_csv(map_filename, index=False)\n",
        "                print(f\"  - Saved site-to-cluster mapping (all noise) to: {map_filename}\")\n",
        "            except KeyError:\n",
        "                print(f\"  - Error: Could not select '{site_id_col}' or 'cluster' for saving map (not enough data).\")\n",
        "            except Exception as e:\n",
        "                print(f\"  - Error saving site-to-cluster map (not enough data): {e}\")\n",
        "        return clustered_dfs # Return early\n",
        "\n",
        "    coords = df[['latitude(sample)', 'longitude(sample)']].values\n",
        "    if np.isnan(coords).any():\n",
        "        # Drop rows with NaN coordinates before clustering\n",
        "        original_len = len(df)\n",
        "        df.dropna(subset=['latitude(sample)', 'longitude(sample)'], inplace=True)\n",
        "        coords = df[['latitude(sample)', 'longitude(sample)']].values\n",
        "        print(f\"  - Warning: Dropped {original_len - len(df)} rows with NaN coordinates before clustering.\")\n",
        "        if len(df) < 2:\n",
        "            print(f\"  - Not enough data points (< 2) after dropping NaNs. Assigning cluster -1.\")\n",
        "            df['cluster'] = -1\n",
        "            clustered_dfs[water_source_label] = df\n",
        "             # Attempt to save mapping even if clustering isn't meaningful (all -1)\n",
        "            if save_mapping_possible and not df.empty:\n",
        "                 try:\n",
        "                     map_df = df[[site_id_col, 'cluster']]\n",
        "                     safe_key = f\"{measurement_label}_{water_source_label}\"\n",
        "                     map_filename = os.path.join(output_dir, f\"site_cluster_map_{safe_key}.csv\")\n",
        "                     map_df.to_csv(map_filename, index=False)\n",
        "                     print(f\"  - Saved site-to-cluster mapping (all noise after NaN drop) to: {map_filename}\")\n",
        "                 except KeyError:\n",
        "                     print(f\"  - Error: Could not select '{site_id_col}' or 'cluster' for saving map (NaN drop).\")\n",
        "                 except Exception as e:\n",
        "                     print(f\"  - Error saving site-to-cluster map (NaN drop): {e}\")\n",
        "            return clustered_dfs # Return early\n",
        "\n",
        "    # --- Perform Clustering ---\n",
        "    dbscan = DBSCAN(eps=eps_km, min_samples=1, metric=haversine_distance_sklearn, algorithm='auto')\n",
        "    try:\n",
        "        cluster_labels = dbscan.fit_predict(coords)\n",
        "        df['cluster'] = cluster_labels # Assign cluster labels\n",
        "    except ValueError as ve:\n",
        "         print(f\"  - Error during DBSCAN clustering: {ve}. Assigning cluster -1.\")\n",
        "         df['cluster'] = -1\n",
        "    except Exception as e:\n",
        "         print(f\"  - Unexpected error during DBSCAN clustering: {e}. Assigning cluster -1.\")\n",
        "         df['cluster'] = -1\n",
        "\n",
        "    n_clusters = len(set(df['cluster'])) - (1 if -1 in df['cluster'].unique() else 0)\n",
        "    n_noise = np.sum(df['cluster'] == -1)\n",
        "    print(f\"  - {n_clusters} cluster(s) found, {n_noise} noise point(s).\")\n",
        "\n",
        "    # ---Save Site ID to Cluster Mapping---\n",
        "    if save_mapping_possible:\n",
        "        try:\n",
        "            # Select only the site ID and the newly assigned cluster\n",
        "            site_cluster_map_df = df[[site_id_col, 'cluster']].copy()\n",
        "            # Remove duplicates if one site_id could appear multiple times in input\n",
        "            # Keep the first occurrence's cluster assignment (usually consistent within clustering step)\n",
        "            site_cluster_map_df.drop_duplicates(subset=[site_id_col], keep='first', inplace=True)\n",
        "\n",
        "            safe_key = f\"{measurement_label}_{water_source_label}\"\n",
        "            map_filename = os.path.join(output_dir, f\"site_cluster_map_{safe_key}.csv\")\n",
        "            site_cluster_map_df.to_csv(map_filename, index=False)\n",
        "            print(f\"  - Saved site-to-cluster mapping to: {map_filename}\")\n",
        "        except KeyError:\n",
        "             print(f\"  - Error: Could not select '{site_id_col}' or 'cluster' columns for saving map.\")\n",
        "        except Exception as e:\n",
        "            print(f\"  - Error saving site-to-cluster map CSV: {e}\")\n",
        "\n",
        "    clustered_dfs[water_source_label] = df\n",
        "    return clustered_dfs\n",
        "\n",
        "\n",
        "# --- Step 6: Calculate Rolling Stats ---\n",
        "def calculate_rolling_stats(\n",
        "    clustered_df: pd.DataFrame, # Receives DataFrame for one water source/measurement type\n",
        "    window_days: int,\n",
        "    measurement_label: Literal['disk', 'tube'],\n",
        "    water_source_label: str # e.g., 'river_stream'\n",
        ") -> Optional[pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Calculates rolling stats for a specific measurement & water source DataFrame.\n",
        "    Uses cluster iteration and sets time index for rolling calculations.\n",
        "    Logs updated labels. The output column 'water_type_group' now contains the water source label.\n",
        "    \"\"\"\n",
        "    print(f\"Step 6 [{measurement_label} / {water_source_label}]: Calculating {window_days}-day rolling statistics...\")\n",
        "    window_str = f'{window_days}D'\n",
        "    df = clustered_df.copy() # Work on a copy\n",
        "\n",
        "    # Define transparency column and source name\n",
        "    transparency_col, source_name = (None, None)\n",
        "    if measurement_label == 'disk':\n",
        "        transparency_col = 'transparencies:transparency disk image disappearance (m)'\n",
        "        source_name = 'disk(m)'\n",
        "    elif measurement_label == 'tube':\n",
        "        transparency_col = 'transparencies:tube image disappearance (cm)'\n",
        "        source_name = 'tube(cm->m)'\n",
        "    else:\n",
        "        print(f\"  - ERROR: calculate_rolling_stats unexpected measurement type '{measurement_label}'. Skipping.\")\n",
        "        return None\n",
        "\n",
        "    # Check if column exists\n",
        "    if transparency_col not in df.columns:\n",
        "        print(f\"  - Designated transparency column '{transparency_col}' not found. Skipping stats calculation.\")\n",
        "        df['measurement_type_group'] = measurement_label\n",
        "        df['water_type_group'] = water_source_label\n",
        "        df['transparency_source'] = 'N/A'\n",
        "        df['transparency_value_unified'] = np.nan\n",
        "        df['rolling_avg'] = np.nan; df['rolling_var'] = np.nan\n",
        "        return df\n",
        "\n",
        "    # Convert 'measured_on' to datetime, handle errors\n",
        "    df['measured_on'] = pd.to_datetime(df['measured_on'], errors='coerce')\n",
        "    original_count = len(df)\n",
        "    df.dropna(subset=['measured_on'], inplace=True)\n",
        "    if original_count > len(df):\n",
        "        print(f\"  - Dropped {original_count - len(df)} rows with invalid dates.\")\n",
        "    if df.empty:\n",
        "        print(f\"  - No valid data points remain after date conversion.\")\n",
        "        return None\n",
        "\n",
        "    # Sort by date\n",
        "    df = df.sort_values('measured_on')\n",
        "\n",
        "    # Prepare result columns\n",
        "    df['transparency_source'] = source_name\n",
        "    df['transparency_value_unified'] = np.nan # Will be in meters\n",
        "    df['rolling_avg'] = np.nan\n",
        "    df['rolling_var'] = np.nan\n",
        "\n",
        "    # Calculate Unified Transparency Value (in meters)\n",
        "    numeric_values = pd.to_numeric(df[transparency_col], errors='coerce')\n",
        "    if measurement_label == 'disk':\n",
        "        df['transparency_value_unified'] = numeric_values\n",
        "    elif measurement_label == 'tube':\n",
        "        df['transparency_value_unified'] = numeric_values / 100.0 # cm to m\n",
        "\n",
        "    # Ensure cluster column is integer\n",
        "    df['cluster'] = pd.to_numeric(df['cluster'], errors='coerce').fillna(-1).astype(int)\n",
        "\n",
        "    # --- Cluster Iteration for Rolling Calculation ---\n",
        "    clusters = sorted([c for c in df['cluster'].unique() if c != -1])\n",
        "    print(f\"  - Processing clusters using iteration: {clusters}\")\n",
        "\n",
        "    calculated_rows_count = 0 # Keep track of rows getting stats\n",
        "\n",
        "    for cluster_id in clusters:\n",
        "        cluster_mask = df['cluster'] == cluster_id\n",
        "        temp_cluster_df = df.loc[cluster_mask].copy() # Create subset for the cluster\n",
        "\n",
        "        if temp_cluster_df.empty:\n",
        "            continue\n",
        "\n",
        "        # Store original indices to assign back results correctly\n",
        "        original_indices_for_cluster = temp_cluster_df.index\n",
        "\n",
        "        # Drop rows *within the cluster* where unified value is NaN before rolling\n",
        "        rows_before_dropna = len(temp_cluster_df)\n",
        "        temp_cluster_df.dropna(subset=['transparency_value_unified'], inplace=True)\n",
        "        rows_after_dropna = len(temp_cluster_df)\n",
        "        valid_original_indices = temp_cluster_df.index # Indices that remain after dropna\n",
        "\n",
        "        if temp_cluster_df.empty:\n",
        "            continue\n",
        "\n",
        "        # --- Perform Rolling Calculations on time index ---\n",
        "        try:\n",
        "            temp_cluster_df_indexed = temp_cluster_df.set_index('measured_on').sort_index()\n",
        "            rolling_obj = temp_cluster_df_indexed['transparency_value_unified'].rolling(window_str, min_periods=1)\n",
        "            rolling_means = rolling_obj.mean()\n",
        "            rolling_vars = rolling_obj.var()\n",
        "\n",
        "            # --- Update the main DataFrame 'df' using original indices ---\n",
        "            mean_map = rolling_means.to_dict()\n",
        "            var_map = rolling_vars.to_dict()\n",
        "            df.loc[valid_original_indices, 'rolling_avg'] = df.loc[valid_original_indices, 'measured_on'].map(mean_map)\n",
        "            df.loc[valid_original_indices, 'rolling_var'] = df.loc[valid_original_indices, 'measured_on'].map(var_map)\n",
        "            calculated_rows_count += len(valid_original_indices)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"    Cluster {int(cluster_id)}: Error during rolling calculation or update: {e}\")\n",
        "            if not valid_original_indices.empty:\n",
        "                 df.loc[valid_original_indices, ['rolling_avg', 'rolling_var']] = np.nan\n",
        "\n",
        "    # --- End Cluster Loop ---\n",
        "\n",
        "    noise_mask = df['cluster'] == -1\n",
        "    if noise_mask.any():\n",
        "        df.loc[noise_mask, ['rolling_avg', 'rolling_var']] = np.nan\n",
        "\n",
        "    print(f\"  - Calculated rolling stats for {calculated_rows_count} out of {len(df)} data points.\") # Report count\n",
        "\n",
        "    if df.empty:\n",
        "        return None\n",
        "    else:\n",
        "        # Add identifying columns back for easier aggregation later\n",
        "        df['measurement_type_group'] = measurement_label\n",
        "        df['water_type_group'] = water_source_label\n",
        "        return df\n",
        "\n",
        "\n",
        "# --- Step 7: Visualize Results ---\n",
        "def visualize_results(result_dfs_dict: Dict[str, pd.DataFrame], output_dir: str):\n",
        "    \"\"\"\n",
        "    Creates plots for each combination of measurement type and water source.\n",
        "    Updates titles and filenames.\n",
        "    \"\"\"\n",
        "    print(\"\\nStep 7: Generating visualizations...\")\n",
        "    if not result_dfs_dict: print(\"  - No results to visualize.\"); return\n",
        "\n",
        "    for key, df in result_dfs_dict.items():\n",
        "        try:\n",
        "             measurement_label, water_source_label = key.split('_', 1)\n",
        "        except ValueError: print(f\"  - Skipping visualization for malformed key: {key}\"); continue\n",
        "\n",
        "        water_source_title = water_source_label.replace('_', ' ').title() # e.g., \"River Stream\"\n",
        "        measurement_title = measurement_label.title()\n",
        "        full_title_prefix = f\"{measurement_title} Data - {water_source_title}\"\n",
        "        safe_key = re.sub(r'\\W+', '_', key)\n",
        "\n",
        "        df['measured_on'] = pd.to_datetime(df['measured_on'], errors='coerce')\n",
        "        plot_df = df[(df['cluster'] != -1) & df['rolling_avg'].notna() & df['measured_on'].notna()].copy()\n",
        "\n",
        "        if plot_df.empty: print(f\"  - [{key}]: No valid data to plot.\"); continue\n",
        "\n",
        "        plot_df = plot_df.sort_values('measured_on')\n",
        "        clusters = sorted(plot_df['cluster'].unique())\n",
        "        source_str = plot_df['transparency_source'].iloc[0] if not plot_df.empty else 'N/A'\n",
        "        y_label_avg = f'Rolling Avg Transparency (m)'; y_label_var = f'Rolling Variance (m^2)'\n",
        "\n",
        "        # Rolling Average Plot\n",
        "        plt.figure(figsize=(15, 8))\n",
        "        for cluster_id in clusters:\n",
        "            cluster_df = plot_df[plot_df['cluster'] == cluster_id]\n",
        "            plt.scatter(cluster_df['measured_on'], cluster_df['rolling_avg'], label=f'Cluster {int(cluster_id)}', alpha=0.7, s=20)\n",
        "        plt.title(f'{ROLLING_WINDOW_DAYS}-Day Rolling Average - {full_title_prefix} (Source: {source_str})')\n",
        "        plt.xlabel('Date'); plt.ylabel(y_label_avg)\n",
        "        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
        "        plt.grid(True, linestyle='--', alpha=0.6); plt.xticks(rotation=45)\n",
        "        plt.tight_layout(rect=[0, 0, 0.85, 1])\n",
        "        plot_filename_avg = os.path.join(output_dir, f\"rolling_avg_{safe_key}.png\")\n",
        "        try: plt.savefig(plot_filename_avg); print(f\"  - Saved plot: {plot_filename_avg}\")\n",
        "        except Exception as e: print(f\"  - Error saving plot {plot_filename_avg}: {e}\")\n",
        "        plt.close()\n",
        "\n",
        "        # Rolling Variance Plot\n",
        "        plot_df_var = plot_df[plot_df['rolling_var'].notna()]\n",
        "        if plot_df_var.empty: print(f\"  - [{key}]: No data with rolling variance to plot.\"); continue\n",
        "        clusters_var = sorted(plot_df_var['cluster'].unique())\n",
        "        plt.figure(figsize=(15, 8))\n",
        "        for cluster_id in clusters_var:\n",
        "            cluster_df = plot_df_var[plot_df_var['cluster'] == cluster_id]\n",
        "            if cluster_df.empty: continue\n",
        "            plt.scatter(cluster_df['measured_on'], cluster_df['rolling_var'], label=f'Cluster {int(cluster_id)}', alpha=0.7, s=20)\n",
        "        plt.title(f'{ROLLING_WINDOW_DAYS}-Day Rolling Variance - {full_title_prefix} (Source: {source_str})')\n",
        "        plt.xlabel('Date'); plt.ylabel(y_label_var)\n",
        "        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
        "        plt.grid(True, linestyle='--', alpha=0.6); plt.xticks(rotation=45)\n",
        "        plt.tight_layout(rect=[0, 0, 0.85, 1])\n",
        "        plot_filename_var = os.path.join(output_dir, f\"rolling_var_{safe_key}.png\")\n",
        "        try: plt.savefig(plot_filename_var); print(f\"  - Saved plot: {plot_filename_var}\")\n",
        "        except Exception as e: print(f\"  - Error saving plot {plot_filename_var}: {e}\")\n",
        "        plt.close()\n",
        "\n",
        "\n",
        "# --- Step 8: Save Results ---\n",
        "def save_results_to_csv(result_dfs_dict: Dict[str, pd.DataFrame], output_dir: str):\n",
        "    \"\"\"\n",
        "    Saves the resulting DataFrames to CSV files, named using the combined key.\n",
        "    Includes the water source column.\n",
        "    \"\"\"\n",
        "    print(\"\\nStep 8: Saving results to CSV...\")\n",
        "    if not result_dfs_dict: print(\"  - No results to save.\"); return\n",
        "\n",
        "    for key, df in result_dfs_dict.items():\n",
        "        safe_key = re.sub(r'\\W+', '_', key)\n",
        "        csv_filename = os.path.join(output_dir, f\"analysis_results_{safe_key}.csv\")\n",
        "        try:\n",
        "            cols_to_save = [\n",
        "                'latitude(sample)', 'longitude(sample)', 'measured_on',\n",
        "                WATER_BODY_SOURCE_COL, # Original water source column\n",
        "                'primary_measurement_type', 'cluster',\n",
        "                'transparency_source', 'transparency_value_unified',\n",
        "                'rolling_avg', 'rolling_var',\n",
        "                'measurement_type_group', # e.g., 'disk', 'tube'\n",
        "                'water_type_group',      # e.g., 'river_stream'\n",
        "                'transparencies:transparency disk image disappearance (m)',\n",
        "                'transparencies:tube image disappearance (cm)'\n",
        "            ]\n",
        "\n",
        "            cols_to_save_present = [col for col in cols_to_save if col in df.columns]\n",
        "            df_to_save = df[cols_to_save_present]\n",
        "            df_to_save.to_csv(csv_filename, index=False)\n",
        "            print(f\"  - Saved results to: {csv_filename}\")\n",
        "        except Exception as e:\n",
        "            print(f\"  - Error saving CSV file {csv_filename}: {e}\")\n",
        "\n",
        "\n",
        "# --- Main Analysis Pipeline ---\n",
        "def run_croatia_water_analysis(\n",
        "    df: pd.DataFrame, cluster_radius_km: float, rolling_window_days: int, output_dir: str\n",
        ") -> Optional[Dict[str, pd.DataFrame]]:\n",
        "    \"\"\"\n",
        "    Runs the complete analysis pipeline:\n",
        "    1. Filters for Croatia.\n",
        "    2. Determines primary measurement type (disk/tube).\n",
        "    3. Separates by measurement type.\n",
        "    4. Separates each measurement type by WATER_BODY_SOURCE_COL.\n",
        "    5. **Filters** to keep only TARGET_WATER_SOURCES (River_Stream, Pond_Lake).\n",
        "    6. Clusters each subset by location.\n",
        "    7. Calculates rolling stats for each cluster.\n",
        "    8. Visualizes and saves results.\n",
        "    \"\"\"\n",
        "    print(\"--- Starting Croatia Water Analysis Pipeline (Disk/Tube Separated, Filtered by Water Source) ---\")\n",
        "    print(f\"Target Water Sources: {TARGET_WATER_SOURCES}\")\n",
        "\n",
        "    croatia_df = filter_croatia_data(df, CROATIA_BOUNDING_BOX)\n",
        "    if croatia_df.empty: print(\"Pipeline Aborted: No data found for Croatia.\"); return None\n",
        "\n",
        "    croatia_df_measured = determine_primary_measurement(croatia_df)\n",
        "    measurement_dfs = separate_by_measurement_type(croatia_df_measured)\n",
        "    if not measurement_dfs: print(\"Pipeline Aborted: No data found for 'disk' or 'tube'.\"); return None\n",
        "\n",
        "    all_final_results = {}\n",
        "    # Convert target source names to safe keys for filtering\n",
        "    target_source_keys = [re.sub(r'\\W+', '_', src.lower()) for src in TARGET_WATER_SOURCES]\n",
        "    print(f\"Safe keys for target sources: {target_source_keys}\")\n",
        "\n",
        "    for measurement_label, measurement_df in measurement_dfs.items():\n",
        "        print(f\"\\n--- Processing Measurement Type: {measurement_label.upper()} ---\")\n",
        "        water_source_dfs = separate_by_water_source(measurement_df, measurement_label)\n",
        "        if not water_source_dfs: print(f\"  - No water sources found for '{measurement_label}'. Skipping.\"); continue\n",
        "\n",
        "        filtered_water_source_dfs = {\n",
        "            key: df_subset for key, df_subset in water_source_dfs.items()\n",
        "            if key in target_source_keys\n",
        "        }\n",
        "\n",
        "        if not filtered_water_source_dfs:\n",
        "            print(f\"  - No data found for target water sources ({', '.join(target_source_keys)}) for '{measurement_label}'. Skipping measurement type.\")\n",
        "            continue\n",
        "        else:\n",
        "             print(f\"  - Focusing analysis on water sources: {list(filtered_water_source_dfs.keys())}\")\n",
        "\n",
        "        # Iterate through the *filtered* water sources\n",
        "        for water_source_label, water_df in filtered_water_source_dfs.items():\n",
        "            print(f\"\\n-- Processing Water Source: {water_source_label} (for {measurement_label}) --\")\n",
        "            current_water_source_dict = {water_source_label: water_df}\n",
        "\n",
        "            clustered_water_source_dict = cluster_by_location(\n",
        "                current_water_source_dict,\n",
        "                cluster_radius_km,\n",
        "                measurement_label,\n",
        "                water_source_label,\n",
        "                output_dir\n",
        "            )\n",
        "\n",
        "            clustered_df = clustered_water_source_dict.get(water_source_label)\n",
        "            # Handle case where clustering itself failed and returned original dict\n",
        "            if clustered_df is None or 'cluster' not in clustered_df.columns:\n",
        "                 print(f\"  - Clustering step did not add 'cluster' column for '{measurement_label}/{water_source_label}'. Skipping subsequent steps.\")\n",
        "                 continue\n",
        "            if clustered_df.empty:\n",
        "                 print(f\"  - DataFrame empty after clustering attempt for '{measurement_label}/{water_source_label}'. Skipping.\")\n",
        "                 continue\n",
        "\n",
        "            result_df = calculate_rolling_stats(clustered_df, rolling_window_days, measurement_label, water_source_label)\n",
        "            if result_df is not None and not result_df.empty:\n",
        "\n",
        "                result_key = f\"{measurement_label}_{water_source_label}\" # e.g., tube_river_stream\n",
        "                all_final_results[result_key] = result_df\n",
        "                print(f\"  - Completed processing for: {result_key}\")\n",
        "            else:\n",
        "                print(f\"  - Rolling stats failed for '{measurement_label}/{water_source_label}'.\")\n",
        "\n",
        "    if not all_final_results: print(\"\\nPipeline Warning: No final results generated for the target water sources.\"); return None\n",
        "    visualize_results(all_final_results, output_dir)\n",
        "    save_results_to_csv(all_final_results, output_dir)\n",
        "    print(\"\\n--- Rolling Statistics Analysis Pipeline Completed ---\")\n",
        "    return all_final_results\n",
        "\n",
        "\n",
        "# --- Bayesian Analysis Function ---\n",
        "def run_bayesian_variance_analysis(\n",
        "    target_df: pd.DataFrame,\n",
        "    target_key: str, # e.g., \"tube_river_stream\"\n",
        "    output_dir: str,\n",
        "    n_draws: int,\n",
        "    n_tune: int,\n",
        "    n_chains: int,\n",
        "    target_accept: float,\n",
        "    ppc_hdi_prob: float = 0.99\n",
        "    ):\n",
        "    \"\"\"\n",
        "    Performs Bayesian analysis on log rolling variance for a given dataset subset,\n",
        "    including map visualization of outliers. Saves identified outlier measurements to CSV.\n",
        "    \"\"\"\n",
        "    print(f\"\\n--- Starting Bayesian Analysis for {target_key} ---\")\n",
        "\n",
        "    # --- 1. Data Preparation ---\n",
        "    print(\"Preparing data for Bayesian analysis...\")\n",
        "    # Ensure Lat/Lon/Source are present for mapping/output later\n",
        "    required_cols_bayes = ['cluster', 'rolling_var', 'latitude(sample)', 'longitude(sample)', 'measured_on', WATER_BODY_SOURCE_COL]\n",
        "    if not all(col in target_df.columns for col in required_cols_bayes):\n",
        "         missing_c = [c for c in required_cols_bayes if c not in target_df.columns]\n",
        "         print(f\"  - ERROR: Missing required columns for Bayesian analysis/mapping in {target_key}: {missing_c}. Skipping.\")\n",
        "         return None\n",
        "\n",
        "    analysis_data = target_df[\n",
        "        (target_df['cluster'] != -1) &\n",
        "        (target_df['rolling_var'].notna()) &\n",
        "        (target_df['rolling_var'] > 0)\n",
        "    ].copy()\n",
        "\n",
        "    if analysis_data.empty:\n",
        "        print(f\"  - No valid data (cluster != -1, rolling_var > 0) found for key '{target_key}'. Skipping Bayesian analysis.\")\n",
        "        return None\n",
        "\n",
        "    analysis_data['log_rolling_var'] = np.log(analysis_data['rolling_var'])\n",
        "\n",
        "    if analysis_data['log_rolling_var'].isnull().any() or np.isinf(analysis_data['log_rolling_var']).any():\n",
        "        print(f\"  - Warning: Found NaN or Inf in log_rolling_var for {target_key}. Removing affected rows.\")\n",
        "        rows_before = len(analysis_data)\n",
        "        analysis_data.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "        analysis_data.dropna(subset=['log_rolling_var'], inplace=True)\n",
        "        print(f\"  - Removed {rows_before - len(analysis_data)} rows with invalid log_rolling_var.\")\n",
        "        if analysis_data.empty:\n",
        "             print(f\"  - No valid log_rolling_var data remains for key '{target_key}'. Skipping Bayesian analysis.\")\n",
        "             return None\n",
        "\n",
        "    unique_clusters = sorted(analysis_data['cluster'].unique())\n",
        "    cluster_map = {cluster_id: i for i, cluster_id in enumerate(unique_clusters)}\n",
        "    analysis_data['cluster_idx'] = analysis_data['cluster'].map(cluster_map)\n",
        "    n_clusters = len(unique_clusters)\n",
        "\n",
        "    if n_clusters == 0: print(f\"  - No clusters with valid data for key '{target_key}'. Skipping.\"); return None\n",
        "    elif n_clusters == 1: print(f\"  - Only one cluster found for key '{target_key}'. Proceeding.\")\n",
        "\n",
        "    log_variance_values = analysis_data['log_rolling_var'].values\n",
        "    cluster_indices = analysis_data['cluster_idx'].values\n",
        "    print(f\"  - Prepared {len(log_variance_values)} data points across {n_clusters} clusters: {unique_clusters}\")\n",
        "    print(f\"  - Log variance range: Min={log_variance_values.min():.3f}, Max={log_variance_values.max():.3f}\")\n",
        "\n",
        "    # --- 2. Define Bayesian Model ---\n",
        "    print(\"\\nDefining Hierarchical Bayesian Model...\")\n",
        "    coords = {\"cluster\": unique_clusters}\n",
        "    with pm.Model(coords=coords) as hierarchical_model:\n",
        "        mu_population = pm.Normal('mu_population', mu=np.median(log_variance_values), sigma=5.)\n",
        "        sigma_population_mu = pm.HalfCauchy('sigma_population_mu', beta=2.5)\n",
        "        sigma_cluster_scale = pm.HalfCauchy('sigma_cluster_scale', beta=2.5)\n",
        "        mu_cluster_offset = pm.Normal('mu_cluster_offset', mu=0., sigma=1., dims=\"cluster\")\n",
        "        mu_cluster = pm.Deterministic('mu_cluster', mu_population + mu_cluster_offset * sigma_population_mu, dims=\"cluster\")\n",
        "        sigma_cluster = pm.HalfCauchy('sigma_cluster', beta=sigma_cluster_scale, dims=\"cluster\")\n",
        "        log_var_obs = pm.Normal('log_var_obs',\n",
        "                                 mu=mu_cluster[cluster_indices],\n",
        "                                 sigma=sigma_cluster[cluster_indices],\n",
        "                                 observed=log_variance_values)\n",
        "    print(\"Model definition complete.\")\n",
        "    try:\n",
        "        graph_img = pm.model_to_graphviz(hierarchical_model)\n",
        "        graph_filename = os.path.join(output_dir, f\"bayes_model_graph_{target_key}.png\")\n",
        "        graph_img.render(graph_filename, format='png', view=False, cleanup=True)\n",
        "        print(f\"  - Saved model graph: {graph_filename}.png\")\n",
        "    except Exception as e: print(f\"  - Could not generate model graph: {e}\")\n",
        "\n",
        "    # --- 3. Run MCMC Sampler ---\n",
        "    print(\"\\nRunning MCMC sampler...\")\n",
        "    print(f\"  - Settings: draws={n_draws}, tune={n_tune}, chains={n_chains}, target_accept={target_accept}\")\n",
        "    idata = None\n",
        "    with hierarchical_model:\n",
        "        try:\n",
        "            idata = pm.sample(draws=n_draws, tune=n_tune, chains=n_chains, cores=1,\n",
        "                               target_accept=target_accept, init='advi+adapt_diag',\n",
        "                               return_inferencedata=True)\n",
        "            print(\"Sampling complete.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error during sampling: {e}. Sampling failed.\")\n",
        "            # Attempt sampling again with jitter+adapt_diag as fallback\n",
        "            print(\"Attempting fallback sampler initialization (jitter+adapt_diag)...\")\n",
        "            try:\n",
        "                 idata = pm.sample(draws=n_draws, tune=n_tune, chains=n_chains, cores=1,\n",
        "                                   target_accept=target_accept, init='jitter+adapt_diag',\n",
        "                                   return_inferencedata=True)\n",
        "                 print(\"Fallback sampling complete.\")\n",
        "            except Exception as e2:\n",
        "                 print(f\"Fallback sampling also failed: {e2}. Bayesian analysis aborted for {target_key}.\")\n",
        "                 return None\n",
        "\n",
        "\n",
        "    # --- 4. Analyze MCMC Results ---\n",
        "    if idata:\n",
        "        print(\"\\n--- Analyzing MCMC Results ---\")\n",
        "        # Convergence Checks\n",
        "        print(\"\\nChecking sampler convergence (R-hat, ESS)...\")\n",
        "        vars_for_summary = ['mu_population', 'sigma_population_mu', 'sigma_cluster_scale', 'mu_cluster', 'sigma_cluster']\n",
        "        summary = az.summary(idata, var_names=vars_for_summary, hdi_prob=0.95)\n",
        "        print(f\"\\nConvergence Summary:\")\n",
        "        print(summary[['mean', 'sd', 'hdi_2.5%', 'hdi_97.5%', 'ess_bulk', 'ess_tail', 'r_hat']].round(3))\n",
        "        rhat_max = summary['r_hat'].max(); ess_bulk_min = summary['ess_bulk'].min(); ess_tail_min = summary['ess_tail'].min()\n",
        "        warnings_convergence = []\n",
        "        if rhat_max > 1.05: warnings_convergence.append(f\"Max R-hat = {rhat_max:.3f} (> 1.05)\")\n",
        "        if ess_bulk_min < 400: warnings_convergence.append(f\"Min Bulk ESS = {ess_bulk_min:.0f} (< 400)\")\n",
        "        if ess_tail_min < 400: warnings_convergence.append(f\"Min Tail ESS = {ess_tail_min:.0f} (< 400)\")\n",
        "        if warnings_convergence:\n",
        "            print(\"\\nWARNINGS:\")\n",
        "            for w in warnings_convergence: print(f\"  - {w}\")\n",
        "            print(\"  Inspect trace plots. Consider increasing tuning or reparameterizing.\")\n",
        "            try:\n",
        "                 az.plot_trace(idata, var_names=vars_for_summary, compact=True, figsize=(15, max(10, len(vars_for_summary)*1.5)))\n",
        "                 trace_filename = os.path.join(output_dir, f\"bayes_trace_plots_{target_key}.png\")\n",
        "                 plt.savefig(trace_filename); print(f\"  - Saved trace plots: {trace_filename}\"); plt.close()\n",
        "            except Exception as e: print(f\"  - Error saving trace plots: {e}\"); plt.close()\n",
        "        else: print(f\"\\nSampler convergence diagnostics look reasonable.\")\n",
        "\n",
        "        # Posterior Analysis for Outlier Clusters\n",
        "        print(\"\\nAnalyzing cluster means (mu_cluster) for potential outliers...\")\n",
        "        # Forest plot\n",
        "        try:\n",
        "            az.plot_forest(idata, var_names=['mu_cluster', 'mu_population'], combined=True, hdi_prob=0.95, figsize=(10, max(5, n_clusters * 0.3)))\n",
        "            plt.title(f'Posterior Means Log Rolling Var (95% HDI)\\nClusters vs Population | Data: {target_key}')\n",
        "            plt.xlabel(\"Mean Log(Rolling Variance)\"); plt.ylabel(\"Parameter / Cluster ID\")\n",
        "            plot_filename_forest = os.path.join(output_dir, f\"bayes_forest_plot_{target_key}.png\")\n",
        "            plt.savefig(plot_filename_forest, bbox_inches='tight'); print(f\"  - Saved forest plot: {plot_filename_forest}\"); plt.close()\n",
        "        except Exception as e: print(f\"Error plotting/saving forest plot: {e}\"); plt.close()\n",
        "\n",
        "        # Delta plot analysis\n",
        "        try:\n",
        "            posterior = idata.posterior\n",
        "            delta_mu = posterior['mu_cluster'] - posterior['mu_population']\n",
        "            idata.posterior['delta_mu'] = delta_mu.assign_coords(cluster=idata.posterior['mu_cluster'].coords['cluster'])\n",
        "            print(\"\\nSummary of Deviations (delta_mu = mu_cluster - mu_population):\")\n",
        "            delta_summary = az.summary(idata, var_names=['delta_mu'], hdi_prob=0.95)\n",
        "            print(delta_summary[['mean', 'sd', 'hdi_2.5%', 'hdi_97.5%']].round(3))\n",
        "            potential_outlier_clusters = []\n",
        "            for cluster_coord in idata.posterior['delta_mu']['cluster'].values:\n",
        "                 # Ensure we handle potential missing keys if summary fails\n",
        "                 idx_label = f'delta_mu[{cluster_coord}]'\n",
        "                 if idx_label in delta_summary.index:\n",
        "                     delta_cluster_summary = delta_summary.loc[idx_label]\n",
        "                     hdi_low = delta_cluster_summary['hdi_2.5%']; hdi_high = delta_cluster_summary['hdi_97.5%']\n",
        "                     if not (hdi_low <= 0 <= hdi_high):\n",
        "                          mean_delta = delta_cluster_summary['mean']\n",
        "                          direction = \"higher\" if mean_delta > 0 else \"lower\"\n",
        "                          potential_outlier_clusters.append({'id': cluster_coord, 'direction': direction, 'mean_delta': mean_delta, 'hdi': f\"[{hdi_low:.3f}, {hdi_high:.3f}]\"})\n",
        "                 else: print(f\"Warning: Could not find summary stats for delta_mu cluster {cluster_coord}\")\n",
        "\n",
        "            if potential_outlier_clusters:\n",
        "                 print(\"\\nPotential Outlier Clusters (95% HDI of delta_mu excludes 0):\")\n",
        "                 for outlier in potential_outlier_clusters: print(f\"  - Cluster {outlier['id']}: Mean log-var tends {outlier['direction']} (Mean Delta: {outlier['mean_delta']:.3f}, HDI: {outlier['hdi']})\")\n",
        "                 outlier_cluster_df = pd.DataFrame(potential_outlier_clusters)\n",
        "                 outlier_cluster_filename = os.path.join(output_dir, f\"bayes_outlier_clusters_{target_key}.csv\")\n",
        "                 outlier_cluster_df.to_csv(outlier_cluster_filename, index=False); print(f\"  - Saved outlier cluster summary to: {outlier_cluster_filename}\")\n",
        "            else: print(\"\\nNo strong evidence for outlier clusters found (based on 95% HDI of delta_mu).\")\n",
        "\n",
        "            az.plot_forest(idata, var_names=['delta_mu'], combined=True, hdi_prob=0.95, figsize=(10, max(5, n_clusters * 0.3)), r_hat=False, ess=False)\n",
        "            plt.axvline(0, color='grey', linestyle='--', label='Population Mean (Delta=0)')\n",
        "            plt.title(f'Deviation of Cluster Mean Log-Var from Pop Mean (95% HDI)\\nData: {target_key}')\n",
        "            plt.xlabel(\"Delta Log(Rolling Variance) [mu_cluster - mu_population]\"); plt.ylabel(\"Cluster ID\")\n",
        "            plt.legend()\n",
        "            plot_filename_delta = os.path.join(output_dir, f\"bayes_delta_plot_{target_key}.png\")\n",
        "            plt.savefig(plot_filename_delta, bbox_inches='tight'); print(f\"  - Saved delta plot: {plot_filename_delta}\"); plt.close()\n",
        "        except Exception as e: print(f\"Error during outlier cluster analysis or plotting: {e}\"); plt.close()\n",
        "\n",
        "        # --- 5. Identify Within-Cluster Data Outliers (PPC) & Map ---\n",
        "        print(\"\\n--- Identifying Within-Cluster Data Outliers (PPC) ---\")\n",
        "        # analysis_data should have lat/lon/source from the check at the start\n",
        "        try:\n",
        "            print(f\"Generating posterior predictive samples (using {len(log_variance_values)} observed points)...\")\n",
        "            with hierarchical_model:\n",
        "                 ppc = pm.sample_posterior_predictive(idata, var_names=[\"log_var_obs\"])\n",
        "            print(\"Posterior predictive sampling complete.\")\n",
        "\n",
        "            observed_log_vars = log_variance_values\n",
        "            ppc_samples_raw = ppc.posterior_predictive['log_var_obs']\n",
        "            # Handle potential issues with stacking if dimensions mismatch\n",
        "            try:\n",
        "                posterior_predictive_samples = ppc_samples_raw.stack(sample=(\"chain\", \"draw\")).values.T\n",
        "            except Exception as stack_err:\n",
        "                 print(f\"Error stacking PPC samples: {stack_err}. Trying alternative reshape.\")\n",
        "                 # Alternative if stack fails (assuming shape is chains, draws, observations)\n",
        "                 n_chains_ppc, n_draws_ppc, n_obs_ppc = ppc_samples_raw.shape\n",
        "                 if n_obs_ppc == len(observed_log_vars):\n",
        "                     posterior_predictive_samples = ppc_samples_raw.values.reshape(-1, n_obs_ppc)\n",
        "                 else:\n",
        "                     print(\"PPC sample observation dimension mismatch. Cannot proceed with outlier check.\")\n",
        "                     posterior_predictive_samples = None # Flag error\n",
        "\n",
        "            outlier_indices_ppc = []\n",
        "            if posterior_predictive_samples is not None:\n",
        "                print(f\"Checking {len(observed_log_vars)} data points against their {ppc_hdi_prob*100:.1f}% Posterior Predictive HDI...\")\n",
        "                for i in range(len(observed_log_vars)):\n",
        "                    obs_val = observed_log_vars[i]\n",
        "                    pred_samples_for_point = posterior_predictive_samples[:, i]\n",
        "                    if np.isnan(pred_samples_for_point).any(): continue\n",
        "                    hdi_pred = az.hdi(pred_samples_for_point, hdi_prob=ppc_hdi_prob)\n",
        "                    if obs_val < hdi_pred[0] or obs_val > hdi_pred[1]:\n",
        "                        outlier_indices_ppc.append(i)\n",
        "\n",
        "            outlier_df_ppc = pd.DataFrame() # Initialize\n",
        "            if outlier_indices_ppc:\n",
        "                print(f\"\\nFound {len(outlier_indices_ppc)} potential within-cluster data outliers (anomalous measurements/sites):\")\n",
        "                # Select the original rows corresponding to these outliers\n",
        "                outlier_df_ppc = analysis_data.iloc[outlier_indices_ppc].copy()\n",
        "\n",
        "                # Calculate and add the HDI interval that the observation fell outside of\n",
        "                hdi_list = []\n",
        "                if posterior_predictive_samples is not None:\n",
        "                    for i in outlier_indices_ppc:\n",
        "                         pred_samples = posterior_predictive_samples[:, i]\n",
        "                         if not np.isnan(pred_samples).any():\n",
        "                            hdi = az.hdi(pred_samples, hdi_prob=ppc_hdi_prob)\n",
        "                            hdi_list.append(f\"[{hdi[0]:.3f}, {hdi[1]:.3f}]\")\n",
        "                         else: hdi_list.append(\"NaN\")\n",
        "                    hdi_col_name = f'ppc_hdi_{int(ppc_hdi_prob*100)}'\n",
        "                    outlier_df_ppc[hdi_col_name] = hdi_list\n",
        "                else: hdi_col_name = 'ppc_hdi_unavailable' # Placeholder if PPC failed\n",
        "\n",
        "                # Include key identifying information for the measurement/site\n",
        "                cols_to_show_save = [\n",
        "                    'cluster', 'measured_on', 'latitude(sample)', 'longitude(sample)',\n",
        "                    WATER_BODY_SOURCE_COL, # Show the specific source\n",
        "                    'rolling_var', 'log_rolling_var'\n",
        "                ]\n",
        "                # Add the HDI column if available\n",
        "                if hdi_col_name in outlier_df_ppc.columns:\n",
        "                    cols_to_show_save.append(hdi_col_name)\n",
        "                # Add original transparency value if helpful and exists\n",
        "                if 'transparency_value_unified' in outlier_df_ppc.columns:\n",
        "                     cols_to_show_save.insert(5, 'transparency_value_unified')\n",
        "                # Add any original ID column if it exists\n",
        "                # id_cols = [c for c in ['id', 'site_id', 'record_id'] if c in outlier_df_ppc.columns]\n",
        "                # cols_to_show_save = id_cols + cols_to_show_save\n",
        "\n",
        "                # Ensure only existing columns are selected\n",
        "                cols_to_show_save_present = [c for c in cols_to_show_save if c in outlier_df_ppc.columns]\n",
        "\n",
        "                print(outlier_df_ppc[cols_to_show_save_present].to_string()) # Print full df content\n",
        "\n",
        "                outlier_filename = os.path.join(output_dir, f\"bayes_anomalous_sites_{target_key}.csv\")\n",
        "                try:\n",
        "                     # Save with original index if it's meaningful, otherwise reset\n",
        "                     outlier_df_ppc[cols_to_show_save_present].to_csv(outlier_filename, index=True)\n",
        "                     print(f\"  - Saved anomalous sites/measurements details to: {outlier_filename}\")\n",
        "                except Exception as e: print(f\"  - Error saving anomalous sites CSV: {e}\")\n",
        "            else:\n",
        "                print(f\"\\nNo within-cluster data outliers (anomalous sites) found based on {ppc_hdi_prob*100:.1f}% Posterior Predictive HDI.\")\n",
        "\n",
        "            # --- Plot Outliers on Map ---\n",
        "            print(\"\\nGenerating map of potential outliers...\")\n",
        "            if not CARTOPY_AVAILABLE:\n",
        "                print(\"  - Cartopy not available, skipping map generation.\")\n",
        "            elif analysis_data.empty:\n",
        "                 print(\"  - No data available for mapping.\")\n",
        "            else:\n",
        "                try:\n",
        "                    plt.figure(figsize=(10, 12))\n",
        "                    ax = plt.axes(projection=ccrs.PlateCarree())\n",
        "                    # Slightly larger extent for Croatia\n",
        "                    ax.set_extent([CROATIA_BOUNDING_BOX['min_lon'] - 1.0, CROATIA_BOUNDING_BOX['max_lon'] + 1.0,\n",
        "                                   CROATIA_BOUNDING_BOX['min_lat'] - 1.0, CROATIA_BOUNDING_BOX['max_lat'] + 1.0],\n",
        "                                  crs=ccrs.PlateCarree())\n",
        "\n",
        "                    # Add map features\n",
        "                    ax.add_feature(cfeature.LAND.with_scale('10m'), facecolor='lightgray', zorder=0)\n",
        "                    ax.add_feature(cfeature.OCEAN.with_scale('10m'), facecolor='lightblue', zorder=0)\n",
        "                    ax.add_feature(cfeature.COASTLINE.with_scale('10m'), edgecolor='black', linewidth=0.5, zorder=2)\n",
        "                    ax.add_feature(cfeature.BORDERS.with_scale('10m'), linestyle=':', edgecolor='gray', zorder=2)\n",
        "                    ax.add_feature(cfeature.LAKES.with_scale('10m'), facecolor='lightblue', edgecolor='black', linewidth=0.2, zorder=1)\n",
        "                    ax.add_feature(cfeature.RIVERS.with_scale('10m'), edgecolor='blue', linewidth=0.3, zorder=1)\n",
        "\n",
        "                    gl = ax.gridlines(crs=ccrs.PlateCarree(), draw_labels=True, linewidth=1, color='gray', alpha=0.5, linestyle='--')\n",
        "                    gl.top_labels = False; gl.right_labels = False\n",
        "\n",
        "                    # Separate non-outliers from outliers within the analysis_data\n",
        "                    if not outlier_df_ppc.empty:\n",
        "                        non_outlier_mask = ~analysis_data.index.isin(outlier_df_ppc.index)\n",
        "                        non_outlier_df = analysis_data[non_outlier_mask]\n",
        "                    else:\n",
        "                        non_outlier_df = analysis_data # All points are non-outliers if none found\n",
        "\n",
        "                    # Plot non-outliers first\n",
        "                    if not non_outlier_df.empty:\n",
        "                        ax.scatter(non_outlier_df['longitude(sample)'], non_outlier_df['latitude(sample)'],\n",
        "                                   color='blue', s=15, alpha=0.6, label='Non-Outlier (in analysis)',\n",
        "                                   transform=ccrs.PlateCarree(), zorder=3)\n",
        "\n",
        "                    # Plot outliers on top\n",
        "                    if not outlier_df_ppc.empty:\n",
        "                        ax.scatter(outlier_df_ppc['longitude(sample)'], outlier_df_ppc['latitude(sample)'],\n",
        "                                   color='red', s=45, alpha=0.9, marker='X', label=f'Potential Outlier ({ppc_hdi_prob*100:.0f}% PPC HDI)',\n",
        "                                   transform=ccrs.PlateCarree(), zorder=4)\n",
        "\n",
        "                    plt.title(f'Potential Within-Cluster Outliers Map\\nData: {target_key}')\n",
        "                    plt.legend(loc='upper right')\n",
        "                    map_filename = os.path.join(output_dir, f\"bayes_outlier_map_{target_key}.png\")\n",
        "                    plt.savefig(map_filename, bbox_inches='tight', dpi=150)\n",
        "                    print(f\"  - Saved outlier map to: {map_filename}\")\n",
        "                    plt.close()\n",
        "                except Exception as e:\n",
        "                    print(f\"  - Error generating or saving outlier map: {e}\")\n",
        "                    plt.close()\n",
        "            # --- End Map Plotting Section ---\n",
        "\n",
        "            # Optional: Plot PPC\n",
        "            try:\n",
        "                az.plot_ppc(idata, num_pp_samples=100)\n",
        "                ppc_filename = os.path.join(output_dir, f\"bayes_ppc_plot_{target_key}.png\")\n",
        "                plt.title(f'Posterior Predictive Check | Data: {target_key}'); plt.savefig(ppc_filename)\n",
        "                print(f\"  - Saved PPC plot: {ppc_filename}\"); plt.close()\n",
        "            except Exception as e: print(f\"  - Error saving PPC plot: {e}\"); plt.close()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error during PPC analysis or mapping: {e}\")\n",
        "            import traceback; traceback.print_exc()\n",
        "\n",
        "    else: # idata is None\n",
        "         print(\"\\nMCMC sampling failed, cannot analyze results, find outliers, or generate map.\")\n",
        "         return None\n",
        "\n",
        "    print(f\"\\n--- Bayesian Analysis Block for {target_key} Complete ---\")\n",
        "    return idata\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BQqMXGMf0z8q",
        "outputId": "72b3f725-ee2b-44c4-fd5f-9654ac9b16cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyMC version: 5.21.2\n",
            "ArviZ version: 0.21.0\n",
            "Cartopy found, map plotting enabled.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Execution ---\n",
        "if __name__ == \"__main__\":\n",
        "    print(f\"Looking for datasets starting with '{DATASET_PREFIX}' in: {FOLDER_PATH}\")\n",
        "    print(f\"Using water source column: '{WATER_BODY_SOURCE_COL}'\")\n",
        "    print(f\"Focusing analysis on sources: {TARGET_WATER_SOURCES}\")\n",
        "\n",
        "    if DATASET_PREFIX == 'globe': effective_prefix = 'globe_water_transparency'\n",
        "    elif DATASET_PREFIX == 'seabass': effective_prefix = 'seabass'\n",
        "    else: print(f\"Error: Unknown DATASET_PREFIX '{DATASET_PREFIX}'.\"); effective_prefix = None\n",
        "\n",
        "    if effective_prefix:\n",
        "        file_path = find_newest_dataset(FOLDER_PATH, effective_prefix)\n",
        "        if file_path:\n",
        "            master_df = load_data(file_path)\n",
        "            if master_df is not None:\n",
        "                # Run rolling stats analysis\n",
        "                analysis_results = run_croatia_water_analysis(\n",
        "                    df=master_df,\n",
        "                    cluster_radius_km=CLUSTER_RADIUS_KM,\n",
        "                    rolling_window_days=ROLLING_WINDOW_DAYS,\n",
        "                    output_dir=OUTPUT_DIR\n",
        "                )\n",
        "\n",
        "                if analysis_results:\n",
        "                    print(\"\\nRolling statistics analysis finished successfully.\")\n",
        "                    # --- Run Bayesian Analysis ---\n",
        "                    if RUN_BAYESIAN_ANALYSIS:\n",
        "                        print(\"\\n--- Starting Bayesian Analysis Phase ---\")\n",
        "                        target_keys_to_analyze = []\n",
        "                        if BAYESIAN_TARGET_KEYS == ['all']:\n",
        "                             target_keys_to_analyze = list(analysis_results.keys())\n",
        "                        else:\n",
        "                            # Ensure target keys are valid based on the filtered results\n",
        "                            target_keys_to_analyze = [k for k in BAYESIAN_TARGET_KEYS if k in analysis_results]\n",
        "                            missing = [k for k in BAYESIAN_TARGET_KEYS if k not in analysis_results and k != 'all']\n",
        "                            if missing: print(f\"Warning: Specified Bayesian target keys not generated by analysis (check source filters?): {missing}\")\n",
        "\n",
        "                        if not target_keys_to_analyze: print(\"No valid target keys found for Bayesian analysis (check config vs available results).\")\n",
        "                        else:\n",
        "                            print(f\"Will attempt Bayesian analysis for keys: {target_keys_to_analyze}\")\n",
        "                            bayes_results = {}\n",
        "                            for target_key in target_keys_to_analyze:\n",
        "                                print(f\"\\n=== Running Bayesian analysis for: {target_key} ===\")\n",
        "                                target_df = analysis_results[target_key]\n",
        "                                idata = run_bayesian_variance_analysis(\n",
        "                                    target_df=target_df, target_key=target_key, output_dir=OUTPUT_DIR,\n",
        "                                    n_draws=BAYESIAN_N_DRAWS, n_tune=BAYESIAN_N_TUNE, n_chains=BAYESIAN_N_CHAINS,\n",
        "                                    target_accept=BAYESIAN_TARGET_ACCEPT, ppc_hdi_prob=BAYESIAN_PPC_HDI_PROB\n",
        "                                )\n",
        "                                if idata: bayes_results[target_key] = idata\n",
        "                                else: print(f\"Bayesian analysis did not complete successfully for {target_key}.\")\n",
        "                        print(\"\\n--- Bayesian Analysis Phase Complete ---\")\n",
        "                    else: print(\"\\nBayesian analysis skipped (config/libs).\")\n",
        "                else: print(\"\\nRolling statistics analysis generated no final data outputs for the target water sources.\")\n",
        "            else: print(\"Failed to load data.\")\n",
        "        else: print(\"No suitable dataset file found.\")\n",
        "    else: print(\"Dataset prefix not set correctly.\")\n",
        "\n",
        "    print(\"\\n--- Full Script Execution Finished ---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "991b0852a5dc4625aa278d021e6839c1",
            "1bdfd8c4812d4106b044b48788f299fa",
            "c3eb31b413b54ba886a157b0c6b67a7f",
            "f07fade6f7734bd6841c1de1255177ef",
            "1a577c0e27fd43ef9cbd2eb36e19b5d6",
            "59c9df66fdda49ca8ee682805e7d8679",
            "883e02307938440cbac30ebc71585823",
            "79f88480febc49ed91c38e2d1fc45a47",
            "b736b84e51a84a3ead09e6e2f37b5bcd",
            "89d3aa776d144421a030a05f22e2193b",
            "4a0d6c10fec4490ab5d90c6eddb83a52",
            "3611b2bd94f34dc7990d53c8f07c924c"
          ]
        },
        "id": "inoUsaiFyy8T",
        "outputId": "8a08a690-5799-4ad0-886e-552a2c5346f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking for datasets starting with 'globe' in: /content/drive/MyDrive/Datasheets\n",
            "Using water source column: 'water_body_source(hes_yellow)'\n",
            "Focusing analysis on sources: ['River_Stream', 'Pond_Lake']\n",
            "Found 1 file(s). Using the newest: 'globe_water_transparency_20250420.csv'.\n",
            "Successfully imported 'globe_water_transparency_20250420.csv'. Shape: (159146, 201)\n",
            "Warning: Some expected columns are missing but not essential for core logic: ['transparencies:transparencydiskimagedisappearance(m)', 'transparencies:tubeimagedisappearance(cm)', 'transparencies:sensorturbidityntu']\n",
            "--- Starting Croatia Water Analysis Pipeline (Disk/Tube Separated, Filtered by Water Source) ---\n",
            "Target Water Sources: ['River_Stream', 'Pond_Lake']\n",
            "Step 1: Filtering data for Croatia...\n",
            "  - Dropped 491 rows with missing/invalid coordinates.\n",
            "  - Found 24885 data points within Croatia's bounding box.\n",
            "Step 2: Determining primary measurement type for each row...\n",
            "  - Primary measurement type counts:\n",
            "    - tube: 15829\n",
            "    - disk: 9056\n",
            "Step 3: Separating data by primary measurement type ('disk', 'tube')...\n",
            "  - Measurement type 'disk': 9056 data points\n",
            "  - Measurement type 'tube': 15829 data points\n",
            "Safe keys for target sources: ['river_stream', 'pond_lake']\n",
            "\n",
            "--- Processing Measurement Type: DISK ---\n",
            "Step 4 [disk]: Separating by water body source ('water_body_source(hes_yellow)')...\n",
            "  - Water body sources found for 'disk': Pond_Lake, River_Stream, Ocean, Marsh, nan\n",
            "    - 'Pond_Lake' (key: 'pond_lake'): 1225 data points\n",
            "    - 'River_Stream' (key: 'river_stream'): 3998 data points\n",
            "    - 'Ocean' (key: 'ocean'): 3384 data points\n",
            "    - 'Marsh' (key: 'marsh'): 350 data points\n",
            "    - 'nan' (key: 'nan'): 99 data points\n",
            "  - Focusing analysis on water sources: ['pond_lake', 'river_stream']\n",
            "\n",
            "-- Processing Water Source: pond_lake (for disk) --\n",
            "Step 5 [disk / pond_lake]: Clustering by location (eps=10 km)...\n",
            "  - 11 cluster(s) found, 0 noise point(s).\n",
            "  - Saved site-to-cluster mapping to: /content/analysis_results_split_source_bayes_map/site_cluster_map_disk_pond_lake.csv\n",
            "Step 6 [disk / pond_lake]: Calculating 5-day rolling statistics...\n",
            "  - Processing clusters using iteration: [np.int64(0), np.int64(1), np.int64(2), np.int64(3), np.int64(4), np.int64(5), np.int64(6), np.int64(7), np.int64(8), np.int64(9), np.int64(10)]\n",
            "  - Calculated rolling stats for 1225 out of 1225 data points.\n",
            "  - Completed processing for: disk_pond_lake\n",
            "\n",
            "-- Processing Water Source: river_stream (for disk) --\n",
            "Step 5 [disk / river_stream]: Clustering by location (eps=10 km)...\n",
            "  - 27 cluster(s) found, 0 noise point(s).\n",
            "  - Saved site-to-cluster mapping to: /content/analysis_results_split_source_bayes_map/site_cluster_map_disk_river_stream.csv\n",
            "Step 6 [disk / river_stream]: Calculating 5-day rolling statistics...\n",
            "  - Processing clusters using iteration: [np.int64(0), np.int64(1), np.int64(2), np.int64(3), np.int64(4), np.int64(5), np.int64(6), np.int64(7), np.int64(8), np.int64(9), np.int64(10), np.int64(11), np.int64(12), np.int64(13), np.int64(14), np.int64(15), np.int64(16), np.int64(17), np.int64(18), np.int64(19), np.int64(20), np.int64(21), np.int64(22), np.int64(23), np.int64(24), np.int64(25), np.int64(26)]\n",
            "  - Calculated rolling stats for 3998 out of 3998 data points.\n",
            "  - Completed processing for: disk_river_stream\n",
            "\n",
            "--- Processing Measurement Type: TUBE ---\n",
            "Step 4 [tube]: Separating by water body source ('water_body_source(hes_yellow)')...\n",
            "  - Water body sources found for 'tube': Marsh, River_Stream, Pond_Lake, Ocean, Insufficient, Non-natural, nan\n",
            "    - 'Marsh' (key: 'marsh'): 1811 data points\n",
            "    - 'River_Stream' (key: 'river_stream'): 10059 data points\n",
            "    - 'Pond_Lake' (key: 'pond_lake'): 2029 data points\n",
            "    - 'Ocean' (key: 'ocean'): 1522 data points\n",
            "    - 'Insufficient' (key: 'insufficient'): 391 data points\n",
            "    - 'Non-natural' (key: 'non_natural'): 15 data points\n",
            "    - 'nan' (key: 'nan'): 2 data points\n",
            "  - Focusing analysis on water sources: ['river_stream', 'pond_lake']\n",
            "\n",
            "-- Processing Water Source: river_stream (for tube) --\n",
            "Step 5 [tube / river_stream]: Clustering by location (eps=10 km)...\n",
            "  - 35 cluster(s) found, 0 noise point(s).\n",
            "  - Saved site-to-cluster mapping to: /content/analysis_results_split_source_bayes_map/site_cluster_map_tube_river_stream.csv\n",
            "Step 6 [tube / river_stream]: Calculating 5-day rolling statistics...\n",
            "  - Processing clusters using iteration: [np.int64(0), np.int64(1), np.int64(2), np.int64(3), np.int64(4), np.int64(5), np.int64(6), np.int64(7), np.int64(8), np.int64(9), np.int64(10), np.int64(11), np.int64(12), np.int64(13), np.int64(14), np.int64(15), np.int64(16), np.int64(17), np.int64(18), np.int64(19), np.int64(20), np.int64(21), np.int64(22), np.int64(23), np.int64(24), np.int64(25), np.int64(26), np.int64(27), np.int64(28), np.int64(29), np.int64(30), np.int64(31), np.int64(32), np.int64(33), np.int64(34)]\n",
            "  - Calculated rolling stats for 10059 out of 10059 data points.\n",
            "  - Completed processing for: tube_river_stream\n",
            "\n",
            "-- Processing Water Source: pond_lake (for tube) --\n",
            "Step 5 [tube / pond_lake]: Clustering by location (eps=10 km)...\n",
            "  - 19 cluster(s) found, 0 noise point(s).\n",
            "  - Saved site-to-cluster mapping to: /content/analysis_results_split_source_bayes_map/site_cluster_map_tube_pond_lake.csv\n",
            "Step 6 [tube / pond_lake]: Calculating 5-day rolling statistics...\n",
            "  - Processing clusters using iteration: [np.int64(0), np.int64(1), np.int64(2), np.int64(3), np.int64(4), np.int64(5), np.int64(6), np.int64(7), np.int64(8), np.int64(9), np.int64(10), np.int64(11), np.int64(12), np.int64(13), np.int64(14), np.int64(15), np.int64(16), np.int64(17), np.int64(18)]\n",
            "  - Calculated rolling stats for 2029 out of 2029 data points.\n",
            "  - Completed processing for: tube_pond_lake\n",
            "\n",
            "Step 7: Generating visualizations...\n",
            "  - Saved plot: /content/analysis_results_split_source_bayes_map/rolling_avg_disk_pond_lake.png\n",
            "  - Saved plot: /content/analysis_results_split_source_bayes_map/rolling_var_disk_pond_lake.png\n",
            "  - Saved plot: /content/analysis_results_split_source_bayes_map/rolling_avg_disk_river_stream.png\n",
            "  - Saved plot: /content/analysis_results_split_source_bayes_map/rolling_var_disk_river_stream.png\n",
            "  - Saved plot: /content/analysis_results_split_source_bayes_map/rolling_avg_tube_river_stream.png\n",
            "  - Saved plot: /content/analysis_results_split_source_bayes_map/rolling_var_tube_river_stream.png\n",
            "  - Saved plot: /content/analysis_results_split_source_bayes_map/rolling_avg_tube_pond_lake.png\n",
            "  - Saved plot: /content/analysis_results_split_source_bayes_map/rolling_var_tube_pond_lake.png\n",
            "\n",
            "Step 8: Saving results to CSV...\n",
            "  - Saved results to: /content/analysis_results_split_source_bayes_map/analysis_results_disk_pond_lake.csv\n",
            "  - Saved results to: /content/analysis_results_split_source_bayes_map/analysis_results_disk_river_stream.csv\n",
            "  - Saved results to: /content/analysis_results_split_source_bayes_map/analysis_results_tube_river_stream.csv\n",
            "  - Saved results to: /content/analysis_results_split_source_bayes_map/analysis_results_tube_pond_lake.csv\n",
            "\n",
            "--- Rolling Statistics Analysis Pipeline Completed ---\n",
            "\n",
            "Rolling statistics analysis finished successfully.\n",
            "\n",
            "--- Starting Bayesian Analysis Phase ---\n",
            "Will attempt Bayesian analysis for keys: ['tube_river_stream', 'disk_pond_lake']\n",
            "\n",
            "=== Running Bayesian analysis for: tube_river_stream ===\n",
            "\n",
            "--- Starting Bayesian Analysis for tube_river_stream ---\n",
            "Preparing data for Bayesian analysis...\n",
            "  - Prepared 3765 data points across 19 clusters: [np.int64(0), np.int64(1), np.int64(2), np.int64(3), np.int64(4), np.int64(8), np.int64(9), np.int64(13), np.int64(16), np.int64(19), np.int64(20), np.int64(21), np.int64(22), np.int64(23), np.int64(24), np.int64(27), np.int64(32), np.int64(33), np.int64(34)]\n",
            "  - Log variance range: Min=-14.509, Max=0.955\n",
            "\n",
            "Defining Hierarchical Bayesian Model...\n",
            "Model definition complete.\n",
            "  - Saved model graph: /content/analysis_results_split_source_bayes_map/bayes_model_graph_tube_river_stream.png.png\n",
            "\n",
            "Running MCMC sampler...\n",
            "  - Settings: draws=2000, tune=2500, chains=4, target_accept=0.9\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "991b0852a5dc4625aa278d021e6839c1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c3eb31b413b54ba886a157b0c6b67a7f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:pymc.stats.convergence:There were 89 divergences after tuning. Increase `target_accept` or reparameterize.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sampling complete.\n",
            "\n",
            "--- Analyzing MCMC Results ---\n",
            "\n",
            "Checking sampler convergence (R-hat, ESS)...\n",
            "\n",
            "Convergence Summary:\n",
            "                      mean     sd  hdi_2.5%  hdi_97.5%  ess_bulk  ess_tail  \\\n",
            "mu_population       -4.727  0.393    -5.584     -3.999    1391.0    2385.0   \n",
            "sigma_population_mu  1.432  0.337     0.894      2.132    1475.0    2250.0   \n",
            "sigma_cluster_scale  2.420  0.662     1.279      3.733    8346.0    5811.0   \n",
            "mu_cluster[0]       -3.736  1.413    -6.557     -1.411    4400.0    5158.0   \n",
            "mu_cluster[1]       -3.995  0.155    -4.303     -3.704    9318.0    5022.0   \n",
            "mu_cluster[2]       -5.827  0.275    -6.369     -5.283   12060.0    5723.0   \n",
            "mu_cluster[3]       -2.837  0.035    -2.907     -2.766    7618.0    6340.0   \n",
            "mu_cluster[4]       -5.663  0.393    -6.429     -4.893   11049.0    5650.0   \n",
            "mu_cluster[8]       -3.602  0.288    -4.188     -3.058   11519.0    5485.0   \n",
            "mu_cluster[9]       -5.733  0.086    -5.910     -5.572    7525.0    5989.0   \n",
            "mu_cluster[13]      -4.548  0.120    -4.777     -4.306    8294.0    6333.0   \n",
            "mu_cluster[16]      -3.444  0.114    -3.678     -3.237    8047.0    6667.0   \n",
            "mu_cluster[19]      -5.758  1.914    -9.803     -2.077    3224.0    1506.0   \n",
            "mu_cluster[20]      -6.518  0.065    -6.642     -6.386    7862.0    5517.0   \n",
            "mu_cluster[21]      -6.826  0.648    -8.040     -5.492    5535.0    4048.0   \n",
            "mu_cluster[22]      -3.893  0.356    -4.630     -3.222    7855.0    4362.0   \n",
            "mu_cluster[23]      -4.157  1.095    -6.391     -2.062    6951.0    5512.0   \n",
            "mu_cluster[24]      -4.228  0.939    -6.139     -2.424    8204.0    5147.0   \n",
            "mu_cluster[27]      -4.658  0.421    -5.525     -3.872   10433.0    5843.0   \n",
            "mu_cluster[32]      -5.170  0.584    -6.270     -3.992    9561.0    5419.0   \n",
            "mu_cluster[33]      -5.526  1.112    -7.634     -3.274    6287.0    5298.0   \n",
            "mu_cluster[34]      -3.811  1.291    -6.488     -1.617    4472.0    5274.0   \n",
            "sigma_cluster[0]     3.423  6.766     0.032      9.890    4115.0    3714.0   \n",
            "sigma_cluster[1]     2.887  0.110     2.672      3.094    9324.0    6302.0   \n",
            "sigma_cluster[2]     2.398  0.199     2.006      2.789    7819.0    5737.0   \n",
            "sigma_cluster[3]     1.359  0.025     1.308      1.407    8379.0    5707.0   \n",
            "sigma_cluster[4]     2.272  0.310     1.743      2.917    8841.0    4998.0   \n",
            "sigma_cluster[8]     2.105  0.207     1.725      2.514    8619.0    5548.0   \n",
            "sigma_cluster[9]     2.488  0.062     2.363      2.605    7716.0    4291.0   \n",
            "sigma_cluster[13]    2.145  0.086     1.978      2.310    8367.0    5524.0   \n",
            "sigma_cluster[16]    2.724  0.079     2.575      2.884    9605.0    6260.0   \n",
            "sigma_cluster[19]    7.858  6.000     0.079     17.810    3135.0    1600.0   \n",
            "sigma_cluster[20]    0.463  0.048     0.372      0.555    8832.0    5229.0   \n",
            "sigma_cluster[21]    1.893  0.559     1.031      2.994    6114.0    4772.0   \n",
            "sigma_cluster[22]    0.822  0.351     0.345      1.483    5872.0    4249.0   \n",
            "sigma_cluster[23]    3.932  1.478     1.736      6.747    7613.0    4765.0   \n",
            "sigma_cluster[24]    4.062  0.965     2.502      5.960    7590.0    4896.0   \n",
            "sigma_cluster[27]    2.822  0.310     2.237      3.431    7784.0    5527.0   \n",
            "sigma_cluster[32]    3.144  0.466     2.312      4.063    8384.0    5255.0   \n",
            "sigma_cluster[33]    4.299  1.273     2.315      6.794    6605.0    4788.0   \n",
            "sigma_cluster[34]    2.878  4.747     0.025      8.633    3626.0    3504.0   \n",
            "\n",
            "                     r_hat  \n",
            "mu_population          1.0  \n",
            "sigma_population_mu    1.0  \n",
            "sigma_cluster_scale    1.0  \n",
            "mu_cluster[0]          1.0  \n",
            "mu_cluster[1]          1.0  \n",
            "mu_cluster[2]          1.0  \n",
            "mu_cluster[3]          1.0  \n",
            "mu_cluster[4]          1.0  \n",
            "mu_cluster[8]          1.0  \n",
            "mu_cluster[9]          1.0  \n",
            "mu_cluster[13]         1.0  \n",
            "mu_cluster[16]         1.0  \n",
            "mu_cluster[19]         1.0  \n",
            "mu_cluster[20]         1.0  \n",
            "mu_cluster[21]         1.0  \n",
            "mu_cluster[22]         1.0  \n",
            "mu_cluster[23]         1.0  \n",
            "mu_cluster[24]         1.0  \n",
            "mu_cluster[27]         1.0  \n",
            "mu_cluster[32]         1.0  \n",
            "mu_cluster[33]         1.0  \n",
            "mu_cluster[34]         1.0  \n",
            "sigma_cluster[0]       1.0  \n",
            "sigma_cluster[1]       1.0  \n",
            "sigma_cluster[2]       1.0  \n",
            "sigma_cluster[3]       1.0  \n",
            "sigma_cluster[4]       1.0  \n",
            "sigma_cluster[8]       1.0  \n",
            "sigma_cluster[9]       1.0  \n",
            "sigma_cluster[13]      1.0  \n",
            "sigma_cluster[16]      1.0  \n",
            "sigma_cluster[19]      1.0  \n",
            "sigma_cluster[20]      1.0  \n",
            "sigma_cluster[21]      1.0  \n",
            "sigma_cluster[22]      1.0  \n",
            "sigma_cluster[23]      1.0  \n",
            "sigma_cluster[24]      1.0  \n",
            "sigma_cluster[27]      1.0  \n",
            "sigma_cluster[32]      1.0  \n",
            "sigma_cluster[33]      1.0  \n",
            "sigma_cluster[34]      1.0  \n",
            "\n",
            "Sampler convergence diagnostics look reasonable.\n",
            "\n",
            "Analyzing cluster means (mu_cluster) for potential outliers...\n",
            "  - Saved forest plot: /content/analysis_results_split_source_bayes_map/bayes_forest_plot_tube_river_stream.png\n",
            "\n",
            "Summary of Deviations (delta_mu = mu_cluster - mu_population):\n",
            "               mean     sd  hdi_2.5%  hdi_97.5%\n",
            "delta_mu[0]   0.991  1.384    -1.701      3.444\n",
            "delta_mu[1]   0.733  0.419    -0.126      1.538\n",
            "delta_mu[2]  -1.100  0.471    -2.013     -0.183\n",
            "delta_mu[3]   1.891  0.395     1.109      2.704\n",
            "delta_mu[4]  -0.936  0.534    -1.968      0.122\n",
            "delta_mu[8]   1.125  0.475     0.226      2.085\n",
            "delta_mu[9]  -1.005  0.401    -1.788     -0.181\n",
            "delta_mu[13]  0.179  0.410    -0.616      1.018\n",
            "delta_mu[16]  1.283  0.407     0.472      2.099\n",
            "delta_mu[19] -1.030  1.841    -5.060      2.452\n",
            "delta_mu[20] -1.790  0.397    -2.601     -1.005\n",
            "delta_mu[21] -2.098  0.713    -3.428     -0.628\n",
            "delta_mu[22]  0.834  0.517    -0.228      1.833\n",
            "delta_mu[23]  0.570  1.093    -1.588      2.741\n",
            "delta_mu[24]  0.499  0.957    -1.398      2.361\n",
            "delta_mu[27]  0.070  0.556    -1.006      1.158\n",
            "delta_mu[32] -0.443  0.671    -1.734      0.870\n",
            "delta_mu[33] -0.799  1.099    -3.023      1.300\n",
            "delta_mu[34]  0.917  1.265    -1.626      3.234\n",
            "\n",
            "Potential Outlier Clusters (95% HDI of delta_mu excludes 0):\n",
            "  - Cluster 2: Mean log-var tends lower (Mean Delta: -1.100, HDI: [-2.013, -0.183])\n",
            "  - Cluster 3: Mean log-var tends higher (Mean Delta: 1.891, HDI: [1.109, 2.704])\n",
            "  - Cluster 8: Mean log-var tends higher (Mean Delta: 1.125, HDI: [0.226, 2.085])\n",
            "  - Cluster 9: Mean log-var tends lower (Mean Delta: -1.005, HDI: [-1.788, -0.181])\n",
            "  - Cluster 16: Mean log-var tends higher (Mean Delta: 1.283, HDI: [0.472, 2.099])\n",
            "  - Cluster 20: Mean log-var tends lower (Mean Delta: -1.790, HDI: [-2.601, -1.005])\n",
            "  - Cluster 21: Mean log-var tends lower (Mean Delta: -2.098, HDI: [-3.428, -0.628])\n",
            "  - Saved outlier cluster summary to: /content/analysis_results_split_source_bayes_map/bayes_outlier_clusters_tube_river_stream.csv\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1a577c0e27fd43ef9cbd2eb36e19b5d6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  - Saved delta plot: /content/analysis_results_split_source_bayes_map/bayes_delta_plot_tube_river_stream.png\n",
            "\n",
            "--- Identifying Within-Cluster Data Outliers (PPC) ---\n",
            "Generating posterior predictive samples (using 3765 observed points)...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Posterior predictive sampling complete.\n",
            "Checking 3765 data points against their 99.0% Posterior Predictive HDI...\n",
            "\n",
            "Found 50 potential within-cluster data outliers (anomalous measurements/sites):\n",
            "        cluster measured_on  latitude(sample)  longitude(sample) water_body_source(hes_yellow)  transparency_value_unified   rolling_var  log_rolling_var        ppc_hdi_99\n",
            "31534         3  2003-10-10         45.495200          15.549700                  River_Stream                       0.330  1.633333e-03        -6.417132   [-6.191, 0.627]\n",
            "38340         3  2004-04-30         45.499100          15.554700                  River_Stream                       0.463  1.104500e-03        -6.808363   [-6.382, 0.591]\n",
            "31094         2  2004-11-16         45.555926          18.683698                  River_Stream                       0.343  4.500000e-06       -12.311433  [-11.787, 0.765]\n",
            "31101         2  2004-12-21         45.555926          18.683698                  River_Stream                       0.567  4.500000e-06       -12.311433  [-11.987, 0.662]\n",
            "38370         3  2005-09-30         45.499100          15.554700                  River_Stream                       0.243  3.645000e-04        -7.916984   [-6.325, 0.677]\n",
            "31654         3  2005-09-30         45.495200          15.549700                  River_Stream                       0.270  3.645000e-04        -7.916984   [-6.391, 0.485]\n",
            "38387         3  2006-02-17         45.499100          15.554700                  River_Stream                       0.460  1.250000e-03        -6.684612   [-6.281, 0.774]\n",
            "38399         3  2006-06-09         45.499100          15.554700                  River_Stream                       0.460  1.250000e-03        -6.684612   [-6.377, 0.619]\n",
            "38438         3  2008-01-18         45.499100          15.554700                  River_Stream                       0.500  5.000000e-05        -9.903488   [-6.452, 0.526]\n",
            "99606         3  2010-12-09         45.484790          15.559890                  River_Stream                       0.300  5.583333e-04        -7.490554   [-6.278, 0.730]\n",
            "99638         3  2010-12-09         45.489010          15.562770                  River_Stream                       0.300  5.583333e-04        -7.490554   [-6.362, 0.619]\n",
            "106687        9  2011-11-18         45.830833          17.374444                  River_Stream                       0.603  4.500000e-06       -12.311433  [-12.101, 0.554]\n",
            "106345        9  2011-11-18         45.831390          17.376940                  River_Stream                       0.600  4.500000e-06       -12.311433  [-12.061, 0.420]\n",
            "104619        3  2011-11-22         45.665250          15.647310                  River_Stream                       0.300  8.000000e-04        -7.130899   [-6.638, 0.538]\n",
            "120225        3  2012-06-04         45.495500          15.541000                  River_Stream                       0.250  1.250000e-03        -6.684612   [-6.352, 0.572]\n",
            "98179         3  2012-10-12         45.495200          15.549700                  River_Stream                       0.530  2.000000e-04        -8.517193   [-6.325, 0.606]\n",
            "120246        3  2012-12-24         45.495500          15.541000                  River_Stream                       0.250  1.333333e-04        -8.922658   [-6.368, 0.512]\n",
            "98692         3  2013-02-06         45.499200          15.551700                  River_Stream                       0.530  1.333333e-04        -8.922658   [-6.140, 0.737]\n",
            "98185         3  2013-02-06         45.495200          15.549700                  River_Stream                       0.530  1.333333e-04        -8.922658   [-6.317, 0.606]\n",
            "104681        3  2013-02-11         45.665250          15.647310                  River_Stream                       0.280  5.000000e-05        -9.903488   [-6.497, 0.465]\n",
            "83211         3  2013-02-26         45.484600          15.559700                  River_Stream                       0.510  1.333333e-04        -8.922658   [-6.264, 0.713]\n",
            "120252        3  2013-03-12         45.495500          15.541000                  River_Stream                       0.250  8.000000e-04        -7.130899   [-6.355, 0.836]\n",
            "104685        3  2013-03-13         45.665250          15.647310                  River_Stream                       0.240  5.000000e-05        -9.903488   [-6.310, 0.691]\n",
            "120261        3  2013-05-09         45.495500          15.541000                  River_Stream                       0.250  2.000000e-04        -8.517193   [-6.356, 0.904]\n",
            "83241         3  2013-09-27         45.484600          15.559700                  River_Stream                       0.510  1.333333e-04        -8.922658   [-6.409, 0.635]\n",
            "83260         3  2014-02-07         45.484600          15.559700                  River_Stream                       0.510  1.333333e-04        -8.922658   [-6.356, 0.673]\n",
            "83261         3  2014-02-28         45.484600          15.559700                  River_Stream                       0.510  2.000000e-04        -8.517193   [-6.422, 0.614]\n",
            "106734        9  2016-03-25         45.830833          17.374444                  River_Stream                       0.480  4.500000e-06       -12.311433  [-12.005, 0.764]\n",
            "106752        9  2016-07-29         45.830833          17.374444                  River_Stream                       0.470  4.500000e-06       -12.311433  [-12.143, 0.920]\n",
            "106998        9  2016-07-29         45.827996          17.375034                  River_Stream                       0.467  4.500000e-06       -12.311433  [-12.098, 0.367]\n",
            "106756        9  2016-08-26         45.830833          17.374444                  River_Stream                       0.530  4.500000e-06       -12.311433  [-12.056, 0.832]\n",
            "107002        9  2016-08-26         45.827996          17.375034                  River_Stream                       0.533  4.500000e-06       -12.311433  [-11.831, 0.836]\n",
            "106765        9  2016-10-21         45.830833          17.374444                  River_Stream                       0.390  4.500000e-06       -12.311433  [-12.116, 0.400]\n",
            "107011        9  2016-10-21         45.827996          17.375034                  River_Stream                       0.393  4.500000e-06       -12.311433  [-12.144, 0.871]\n",
            "107013        9  2016-11-04         45.827996          17.375034                  River_Stream                       0.523  4.500000e-06       -12.311433  [-12.213, 0.515]\n",
            "106767        9  2016-11-04         45.830833          17.374444                  River_Stream                       0.520  4.500000e-06       -12.311433  [-12.110, 0.649]\n",
            "115950       13  2017-01-10         45.444830          16.270096                  River_Stream                       0.990  5.000000e-05        -9.903488   [-9.863, 1.033]\n",
            "106780        9  2017-02-03         45.830833          17.374444                  River_Stream                       0.620  4.500000e-06       -12.311433  [-12.006, 0.720]\n",
            "104141       20  2017-03-31         46.025456          15.913246                  River_Stream                       0.900  5.000000e-03        -5.298317  [-7.776, -5.325]\n",
            "104242       20  2017-06-23         46.020390          15.905049                  River_Stream                       0.500  5.000000e-03        -5.298317  [-7.728, -5.334]\n",
            "104153       20  2017-06-23         46.025456          15.913246                  River_Stream                       0.600  5.000000e-03        -5.298317  [-7.787, -5.301]\n",
            "106467        9  2017-11-18         45.831390          17.376940                  River_Stream                       0.783  2.250000e-06       -13.004580  [-11.920, 0.537]\n",
            "106824        9  2017-11-18         45.830833          17.374444                  River_Stream                       0.783  2.250000e-06       -13.004580  [-12.561, 0.362]\n",
            "106468        9  2017-11-18         45.831390          17.376940                  River_Stream                       0.780  2.250000e-06       -13.004580  [-11.958, 0.569]\n",
            "107060        9  2017-11-18         45.827996          17.375034                  River_Stream                       0.783  2.250000e-06       -13.004580  [-12.201, 0.570]\n",
            "115738       32  2018-02-14         45.555500          16.436900                  River_Stream                       0.630  5.000000e-07       -14.508658  [-13.546, 3.876]\n",
            "98431         3  2019-10-18         45.495200          15.549700                  River_Stream                       0.510  5.000000e-05        -9.903488   [-6.228, 0.692]\n",
            "98486         3  2021-03-08         45.495200          15.549700                  River_Stream                       0.510  1.980050e+00         0.683122   [-6.259, 0.574]\n",
            "105458       27  2024-10-23         46.416084          15.872424                  River_Stream                       0.610  4.500000e-06       -12.311433  [-12.196, 3.080]\n",
            "98645         3  2024-12-04         45.495200          15.549700                  River_Stream                       0.510  5.000000e-05        -9.903488   [-6.291, 0.644]\n",
            "  - Saved anomalous sites/measurements details to: /content/analysis_results_split_source_bayes_map/bayes_anomalous_sites_tube_river_stream.csv\n",
            "\n",
            "Generating map of potential outliers...\n",
            "  - Saved outlier map to: /content/analysis_results_split_source_bayes_map/bayes_outlier_map_tube_river_stream.png\n",
            "  - Error saving PPC plot: `data` argument must have the group \"posterior_predictive\" for ppcplot\n",
            "\n",
            "--- Bayesian Analysis Block for tube_river_stream Complete ---\n",
            "\n",
            "=== Running Bayesian analysis for: disk_pond_lake ===\n",
            "\n",
            "--- Starting Bayesian Analysis for disk_pond_lake ---\n",
            "Preparing data for Bayesian analysis...\n",
            "  - Prepared 39 data points across 6 clusters: [np.int64(0), np.int64(1), np.int64(2), np.int64(3), np.int64(5), np.int64(6)]\n",
            "  - Log variance range: Min=-9.903, Max=0.118\n",
            "\n",
            "Defining Hierarchical Bayesian Model...\n",
            "Model definition complete.\n",
            "  - Saved model graph: /content/analysis_results_split_source_bayes_map/bayes_model_graph_disk_pond_lake.png.png\n",
            "\n",
            "Running MCMC sampler...\n",
            "  - Settings: draws=2000, tune=2500, chains=4, target_accept=0.9\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "883e02307938440cbac30ebc71585823"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b736b84e51a84a3ead09e6e2f37b5bcd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sampling complete.\n",
            "\n",
            "--- Analyzing MCMC Results ---\n",
            "\n",
            "Checking sampler convergence (R-hat, ESS)...\n",
            "\n",
            "Convergence Summary:\n",
            "                      mean     sd  hdi_2.5%  hdi_97.5%  ess_bulk  ess_tail  \\\n",
            "mu_population       -4.347  0.990    -6.429     -2.407    2785.0    3098.0   \n",
            "sigma_population_mu  2.010  0.934     0.456      3.951    2670.0    2553.0   \n",
            "sigma_cluster_scale  2.852  1.349     0.831      5.540    8103.0    5831.0   \n",
            "mu_cluster[0]       -5.792  0.705    -7.166     -4.355    9043.0    6650.0   \n",
            "mu_cluster[1]       -5.534  1.134    -7.692     -3.239    7100.0    5351.0   \n",
            "mu_cluster[2]       -2.715  1.042    -5.019     -0.842    5147.0    3335.0   \n",
            "mu_cluster[3]       -3.679  1.259    -6.177     -1.201    8210.0    6132.0   \n",
            "mu_cluster[5]       -5.527  0.938    -7.368     -3.635    7897.0    5896.0   \n",
            "mu_cluster[6]       -2.828  1.239    -5.291     -0.439    5725.0    4681.0   \n",
            "sigma_cluster[0]     2.118  0.631     1.190      3.374    9076.0    4736.0   \n",
            "sigma_cluster[1]     2.554  1.406     0.806      4.928    6510.0    4437.0   \n",
            "sigma_cluster[2]     2.211  1.078     0.906      4.346    5850.0    4511.0   \n",
            "sigma_cluster[3]     3.741  1.323     1.886      6.238    7979.0    4873.0   \n",
            "sigma_cluster[5]     2.681  0.868     1.375      4.358    7899.0    5152.0   \n",
            "sigma_cluster[6]     3.730  1.149     2.010      6.083    7119.0    4769.0   \n",
            "\n",
            "                     r_hat  \n",
            "mu_population          1.0  \n",
            "sigma_population_mu    1.0  \n",
            "sigma_cluster_scale    1.0  \n",
            "mu_cluster[0]          1.0  \n",
            "mu_cluster[1]          1.0  \n",
            "mu_cluster[2]          1.0  \n",
            "mu_cluster[3]          1.0  \n",
            "mu_cluster[5]          1.0  \n",
            "mu_cluster[6]          1.0  \n",
            "sigma_cluster[0]       1.0  \n",
            "sigma_cluster[1]       1.0  \n",
            "sigma_cluster[2]       1.0  \n",
            "sigma_cluster[3]       1.0  \n",
            "sigma_cluster[5]       1.0  \n",
            "sigma_cluster[6]       1.0  \n",
            "\n",
            "Sampler convergence diagnostics look reasonable.\n",
            "\n",
            "Analyzing cluster means (mu_cluster) for potential outliers...\n",
            "  - Saved forest plot: /content/analysis_results_split_source_bayes_map/bayes_forest_plot_disk_pond_lake.png\n",
            "\n",
            "Summary of Deviations (delta_mu = mu_cluster - mu_population):\n",
            "              mean     sd  hdi_2.5%  hdi_97.5%\n",
            "delta_mu[0] -1.446  1.126    -3.707      0.748\n",
            "delta_mu[1] -1.187  1.337    -3.961      1.338\n",
            "delta_mu[2]  1.631  1.249    -0.890      4.023\n",
            "delta_mu[3]  0.667  1.357    -1.969      3.512\n",
            "delta_mu[5] -1.181  1.231    -3.679      1.196\n",
            "delta_mu[6]  1.519  1.349    -1.023      4.214\n",
            "\n",
            "No strong evidence for outlier clusters found (based on 95% HDI of delta_mu).\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4a0d6c10fec4490ab5d90c6eddb83a52"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  - Saved delta plot: /content/analysis_results_split_source_bayes_map/bayes_delta_plot_disk_pond_lake.png\n",
            "\n",
            "--- Identifying Within-Cluster Data Outliers (PPC) ---\n",
            "Generating posterior predictive samples (using 39 observed points)...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Posterior predictive sampling complete.\n",
            "Checking 39 data points against their 99.0% Posterior Predictive HDI...\n",
            "\n",
            "No within-cluster data outliers (anomalous sites) found based on 99.0% Posterior Predictive HDI.\n",
            "\n",
            "Generating map of potential outliers...\n",
            "  - Saved outlier map to: /content/analysis_results_split_source_bayes_map/bayes_outlier_map_disk_pond_lake.png\n",
            "  - Error saving PPC plot: `data` argument must have the group \"posterior_predictive\" for ppcplot\n",
            "\n",
            "--- Bayesian Analysis Block for disk_pond_lake Complete ---\n",
            "\n",
            "--- Bayesian Analysis Phase Complete ---\n",
            "\n",
            "--- Full Script Execution Finished ---\n"
          ]
        }
      ]
    }
  ]
}